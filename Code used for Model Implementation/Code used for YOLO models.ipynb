{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10863715,
          "sourceType": "datasetVersion",
          "datasetId": 6572001
        },
        {
          "sourceId": 11172942,
          "sourceType": "datasetVersion",
          "datasetId": 6740068
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "yaml_content = \"\"\"train: /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images\n",
        "val: /kaggle/input/shoe-data/Data-defect/Data-defect/val/images\n",
        "test: /kaggle/input/shoe-data/Data-defect/Data-defect/test-def/images\n",
        "nc: 1\n",
        "names: ['Defect']\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/kaggle/working/data.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"data.yaml created in /kaggle/working/\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T11:42:32.378635Z",
          "iopub.execute_input": "2025-04-14T11:42:32.378857Z",
          "iopub.status.idle": "2025-04-14T11:42:32.387253Z",
          "shell.execute_reply.started": "2025-04-14T11:42:32.378830Z",
          "shell.execute_reply": "2025-04-14T11:42:32.386604Z"
        },
        "id": "w6ZSKqj5SXtR",
        "outputId": "8fd9834a-a795-404d-b8ca-f902f67cb689"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "data.yaml created in /kaggle/working/\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo"
      ],
      "metadata": {
        "id": "vkZxQGe73vnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "metadata": {
        "id": "VKKTmQhc17hg",
        "outputId": "6c346f82-998e-4966-d34f-85a56bec779f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T08:59:43.983925Z",
          "iopub.execute_input": "2025-04-14T08:59:43.984453Z",
          "iopub.status.idle": "2025-04-14T08:59:45.694115Z",
          "shell.execute_reply.started": "2025-04-14T08:59:43.984430Z",
          "shell.execute_reply": "2025-04-14T08:59:45.693227Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Cloning into 'yolov5'...\nremote: Enumerating objects: 17372, done.\u001b[K\nremote: Counting objects: 100% (59/59), done.\u001b[K\nremote: Compressing objects: 100% (39/39), done.\u001b[K\nremote: Total 17372 (delta 42), reused 20 (delta 20), pack-reused 17313 (from 3)\u001b[K\nReceiving objects: 100% (17372/17372), 16.25 MiB | 34.10 MiB/s, done.\nResolving deltas: 100% (11910/11910), done.\n/kaggle/working/yolov5\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "ImJV1DvqFOud",
        "outputId": "deb513ae-4731-4615-c9a9-5f79a7cd9ab7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T11:44:16.176195Z",
          "iopub.execute_input": "2025-04-14T11:44:16.176852Z",
          "iopub.status.idle": "2025-04-14T11:45:28.285656Z",
          "shell.execute_reply.started": "2025-04-14T11:44:16.176824Z",
          "shell.execute_reply": "2025-04-14T11:45:28.284656Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting ultralytics\n  Downloading ultralytics-8.3.107-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.107-py3-none-any.whl (974 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m974.5/974.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.107 ultralytics-thop-2.0.14\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolov5s"
      ],
      "metadata": {
        "id": "Szs6odb_l97i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Path to your dataset's YAML file\n",
        "data_yaml_path = '/kaggle/working/data.yaml'  # Ensure this YAML file is correctly configured\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T11:45:28.287167Z",
          "iopub.execute_input": "2025-04-14T11:45:28.287494Z",
          "iopub.status.idle": "2025-04-14T11:45:32.915426Z",
          "shell.execute_reply.started": "2025-04-14T11:45:28.287470Z",
          "shell.execute_reply": "2025-04-14T11:45:32.914865Z"
        },
        "id": "4-jTu0tHSXtb",
        "outputId": "d5d7c50e-169f-4e46-8294-cd3fb5a58fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder containing your test .txt label files\n",
        "folder_path = '/kaggle/input/shoe-data/Data-defect/Data-defect/test-def/labels'  # Replace with your actual folder path\n",
        "\n",
        "empty_count = 0\n",
        "non_empty_count = 0\n",
        "\n",
        "# Loop through all .txt files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read().strip()\n",
        "\n",
        "            if content == '':\n",
        "                empty_count += 1\n",
        "            else:\n",
        "                non_empty_count += 1\n",
        "\n",
        "print(f\"Empty files: {empty_count}\")\n",
        "print(f\"Files with content: {non_empty_count}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T09:02:03.185055Z",
          "iopub.execute_input": "2025-04-14T09:02:03.185951Z",
          "iopub.status.idle": "2025-04-14T09:02:03.796729Z",
          "shell.execute_reply.started": "2025-04-14T09:02:03.185923Z",
          "shell.execute_reply": "2025-04-14T09:02:03.795887Z"
        },
        "id": "Zf37QesnSXtc",
        "outputId": "3ffe2451-a3a2-44b2-fcaa-4c3a20993588"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Empty files: 88\nFiles with content: 92\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Path to your dataset's YAML file\n",
        "data_yaml_path = '/kaggle/working/data.yaml'  # Ensure this YAML file is correctly configured\n",
        "\n",
        "# Load YOLOv8 model with pre-trained weights\n",
        "model = YOLO(\"yolov8s.pt\")  #"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T12:05:09.011215Z",
          "iopub.execute_input": "2025-04-14T12:05:09.011852Z",
          "iopub.status.idle": "2025-04-14T12:05:11.445475Z",
          "shell.execute_reply.started": "2025-04-14T12:05:09.011822Z",
          "shell.execute_reply": "2025-04-14T12:05:11.444497Z"
        },
        "id": "7BXfn_kySXtd",
        "outputId": "05f37b1f-6877-4d97-b3f2-f86122308947"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.5M/21.5M [00:00<00:00, 40.9MB/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T12:05:16.585910Z",
          "iopub.execute_input": "2025-04-14T12:05:16.586187Z",
          "iopub.status.idle": "2025-04-14T12:05:16.593455Z",
          "shell.execute_reply.started": "2025-04-14T12:05:16.586164Z",
          "shell.execute_reply": "2025-04-14T12:05:16.592892Z"
        },
        "id": "Y7Vfjuq0SXte",
        "outputId": "d0dde163-8f0d-4835-9d5d-2758f2b83807"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "YOLO(\n  (model): DetectionModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (4): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-1): 2 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (6): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-1): 2 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (8): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): Upsample(scale_factor=2.0, mode='nearest')\n      (11): Concat()\n      (12): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (13): Upsample(scale_factor=2.0, mode='nearest')\n      (14): Concat()\n      (15): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (16): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (17): Concat()\n      (18): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (19): Conv(\n        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (20): Concat()\n      (21): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (22): Detect(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Path to your dataset's YAML file\n",
        "data_yaml_path = '/kaggle/working/data.yaml'  # Ensure this YAML file is correctly configured\n",
        "\n",
        "# Load YOLOv8 model with pre-trained weights\n",
        "model = YOLO(\"yolov5s.pt\")  # Using YOLOv8 weights\n",
        "\n",
        "# Train the model\n",
        "model.train(\n",
        "    data=data_yaml_path,\n",
        "    epochs=30,               # More training\n",
        "    batch=32,                # Larger batch\n",
        "    imgsz=960,               # Higher resolution\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    lr0=0.001, lrf=0.01,      # Learning rate decay\n",
        "    optimizer='SGD',         # Try SGD for detection tasks\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005,\n",
        "    cos_lr=True,             # Cyclic LR\n",
        "    augment=True,\n",
        "    freeze=[0, 1, 2],\n",
        "    amp=True                 # Enable mixed precision\n",
        ")"
      ],
      "metadata": {
        "id": "jL3jfNVJnWOo",
        "outputId": "63d4ab81-f67c-47c3-b354-bd2689f38e88",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T09:03:13.375890Z",
          "iopub.execute_input": "2025-04-14T09:03:13.376510Z",
          "iopub.status.idle": "2025-04-14T09:47:47.005516Z",
          "shell.execute_reply.started": "2025-04-14T09:03:13.376483Z",
          "shell.execute_reply": "2025-04-14T09:47:47.004115Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PRO TIP ğŸ’¡ Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\nYOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5su.pt to 'yolov5su.pt'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7M/17.7M [00:00<00:00, 134MB/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.107 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5s.pt, data=/kaggle/working/data.yaml, epochs=30, time=None, patience=100, batch=32, imgsz=960, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=[0, 1, 2], multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 27.7MB/s]\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744621400.430821      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744621400.482958      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Overriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \nYOLOv5s summary: 153 layers, 9,122,579 parameters, 9,122,563 gradients, 24.0 GFLOPs\n\nTransferred 421/427 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\nFreezing layer 'model.0.conv.weight'\nFreezing layer 'model.0.bn.weight'\nFreezing layer 'model.0.bn.bias'\nFreezing layer 'model.1.conv.weight'\nFreezing layer 'model.1.bn.weight'\nFreezing layer 'model.1.bn.bias'\nFreezing layer 'model.2.cv1.conv.weight'\nFreezing layer 'model.2.cv1.bn.weight'\nFreezing layer 'model.2.cv1.bn.bias'\nFreezing layer 'model.2.cv2.conv.weight'\nFreezing layer 'model.2.cv2.bn.weight'\nFreezing layer 'model.2.cv2.bn.bias'\nFreezing layer 'model.2.cv3.conv.weight'\nFreezing layer 'model.2.cv3.bn.weight'\nFreezing layer 'model.2.cv3.bn.bias'\nFreezing layer 'model.2.m.0.cv1.conv.weight'\nFreezing layer 'model.2.m.0.cv1.bn.weight'\nFreezing layer 'model.2.m.0.cv1.bn.bias'\nFreezing layer 'model.2.m.0.cv2.conv.weight'\nFreezing layer 'model.2.m.0.cv2.bn.weight'\nFreezing layer 'model.2.m.0.cv2.bn.bias'\nFreezing layer 'model.24.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 118MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/labels... 1440 images, 720 backgrounds, 4 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1440/1440 [00:10<00:00, 135.29it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_28_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_437_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_732_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_896_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/train-def is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:01<00:00, 122.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\nPlotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.001, momentum=0.937) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 960 train, 960 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 30 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       1/30      11.6G      3.589      9.378      3.429         39        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:25<00:00,  1.89s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.76s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116   0.000975      0.431    0.00182   0.000331\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       2/30      11.6G      2.802      3.904      2.974         29        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:21<00:00,  1.81s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.61it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.21      0.112      0.119     0.0263\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       3/30      11.6G      2.267      2.931      2.471         24        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:22<00:00,  1.84s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.35it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.289      0.266      0.198      0.074\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       4/30      11.6G      2.037      2.361      2.155         23        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:18<00:00,  1.76s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.45it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.44      0.483      0.461      0.189\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       5/30      11.6G      1.901      1.982       2.02         32        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:24<00:00,  1.88s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.42it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.64      0.517      0.611      0.267\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       6/30      11.6G      1.813      1.752       1.91         26        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:26<00:00,  1.92s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.69it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.695      0.708      0.721      0.335\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       7/30      11.6G      1.734      1.627       1.85         34        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:22<00:00,  1.84s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.30it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.697      0.693      0.758      0.374\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       8/30      11.6G      1.649      1.509      1.772         26        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:22<00:00,  1.84s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.47it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.89      0.733      0.827      0.452\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       9/30      11.6G      1.596      1.361      1.683         31        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:24<00:00,  1.88s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.47it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.806      0.823       0.85      0.444\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      10/30      11.6G      1.548      1.236      1.645         24        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:25<00:00,  1.89s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.81it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.887      0.784      0.872      0.485\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      11/30      11.6G      1.489      1.159      1.584         26        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:21<00:00,  1.81s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.59it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.857      0.845      0.888       0.48\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      12/30      11.6G      1.433      1.092      1.507         35        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:23<00:00,  1.86s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.60it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.849      0.822      0.885      0.476\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      13/30      11.6G      1.397      1.049      1.466         46        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:21<00:00,  1.81s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.35it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.889      0.862      0.916      0.529\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      14/30      11.6G      1.333     0.9999      1.414         29        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:25<00:00,  1.91s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.864      0.824      0.883      0.517\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      15/30      11.6G      1.301      0.959      1.391         28        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:23<00:00,  1.86s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.848      0.845      0.913      0.492\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      16/30      11.6G      1.259     0.9016      1.339         26        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:23<00:00,  1.85s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.52it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.876      0.879      0.937      0.555\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      17/30      11.6G      1.262     0.8741       1.33         32        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:23<00:00,  1.85s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.45it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.83      0.888      0.926      0.526\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      18/30      11.6G       1.22     0.8697      1.328         37        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:24<00:00,  1.88s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.45it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.892      0.879      0.946      0.574\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      19/30      11.6G      1.195     0.8408       1.29         27        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:21<00:00,  1.81s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.49it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.842      0.931      0.942      0.589\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      20/30      11.6G      1.197     0.8353      1.279         28        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:21<00:00,  1.80s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.37it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.892      0.905       0.94      0.599\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      21/30      11.6G      1.184     0.7329      1.307         17        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:30<00:00,  2.01s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.40it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.901      0.879      0.933      0.595\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      22/30      11.6G       1.12      0.684      1.245         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:21<00:00,  1.82s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.41it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.914      0.905      0.956      0.581\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      23/30      11.6G      1.101      0.684      1.241         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:22<00:00,  1.84s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.64it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.932      0.944       0.96      0.616\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      24/30      11.6G      1.054     0.6473      1.202          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:20<00:00,  1.80s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.915      0.948       0.96      0.626\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      25/30      11.6G      1.024     0.6146      1.187         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:19<00:00,  1.76s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.62it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.909      0.948      0.957      0.614\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      26/30      11.6G      1.015     0.6243      1.183         17        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:20<00:00,  1.78s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.924      0.945      0.958       0.61\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      27/30      11.6G     0.9899     0.6038      1.171         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:19<00:00,  1.78s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.56it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.93       0.92      0.962      0.591\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      28/30      11.6G      1.012     0.6174      1.175         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:23<00:00,  1.86s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.44it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.931      0.948      0.964      0.625\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      29/30      11.6G     0.9955     0.6085      1.163         22        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:22<00:00,  1.84s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.69it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.923       0.94      0.962      0.601\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      30/30      11.6G     0.9874     0.5978      1.161         23        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [01:18<00:00,  1.74s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.62it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.921       0.94      0.963      0.615\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n30 epochs completed in 0.722 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 18.5MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 18.5MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics 8.3.107 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nYOLOv5s summary (fused): 84 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.15s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.92      0.887      0.957      0.668\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 0.2ms preprocess, 24.2ms inference, 0.0ms loss, 3.0ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "ultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([0])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7b18d0ed1050>\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.98947,\n            0.98947,     0.98947,     0.98947,     0.98947,     0.98947,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,\n            0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,\n            0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.96154,     0.94393,     0.94393,     0.94393,     0.94393,     0.94393,     0.94393,     0.94393,     0.94393,     0.93578,     0.93578,     0.93578,     0.93578,\n            0.93578,     0.93578,     0.93578,     0.93578,     0.93578,     0.91964,     0.91964,     0.91964,     0.91964,     0.91964,     0.91964,     0.91964,     0.91964,     0.91964,     0.90435,     0.90435,     0.90435,     0.90435,     0.90435,     0.90435,     0.90435,     0.90435,     0.89744,\n            0.89744,     0.89744,     0.89744,     0.89744,     0.89744,     0.89744,     0.89744,     0.89744,     0.87603,     0.87603,     0.87603,     0.87603,     0.87603,     0.87603,     0.87603,     0.87603,      0.8629,      0.8629,      0.8629,      0.8629,      0.8629,      0.8629,      0.8629,\n             0.8629,      0.8629,     0.81818,     0.81818,     0.81818,     0.81818,     0.81818,     0.81818,     0.81818,     0.81818,     0.81818,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.81481,\n            0.81481,     0.81481,     0.81481,     0.81481,     0.81481,     0.68098,     0.68098,     0.68098,     0.68098,     0.68098,     0.68098,     0.68098,     0.68098,     0.57732,     0.57732,     0.57732,     0.57732,     0.57732,     0.57732,     0.57732,     0.57732,     0.57732,     0.45749,\n            0.45749,     0.45749,     0.45749,     0.45749,     0.45749,     0.45749,     0.45749,     0.45749,      0.3931,      0.3931,      0.3931,      0.3931,      0.3931,      0.3931,      0.3931,      0.3931,     0.23279,     0.23279,     0.23279,     0.23279,     0.23279,     0.23279,     0.23279,\n            0.23279,     0.23279,     0.11058,     0.11058,     0.11058,     0.11058,     0.11058,     0.11058,     0.11058,     0.11058,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.15644,     0.15644,     0.23372,     0.29279,     0.32484,     0.35582,     0.37443,     0.39036,     0.41385,     0.43475,     0.44971,     0.45739,      0.4716,     0.48542,     0.49261,     0.49929,     0.50552,     0.51762,     0.53204,     0.53927,     0.54995,      0.5547,     0.55951,\n            0.56115,     0.56753,      0.5728,     0.57899,     0.58227,     0.58609,     0.59264,     0.59877,     0.60254,      0.6061,     0.60858,     0.61531,     0.61755,     0.61939,     0.62038,     0.62003,     0.62344,     0.62464,      0.6256,     0.62788,     0.63282,      0.6369,     0.63918,\n            0.64076,     0.64166,      0.6446,     0.64618,     0.65358,     0.65472,     0.65651,     0.65742,     0.65807,     0.65872,     0.65998,     0.66334,     0.66496,     0.66543,     0.66591,     0.66639,     0.66707,     0.66803,     0.66968,     0.67128,     0.67219,     0.67669,     0.68046,\n            0.68143,     0.68219,     0.68311,     0.68648,     0.68894,     0.69223,     0.69412,     0.69484,     0.69556,      0.6987,     0.70064,     0.70679,     0.70734,     0.70789,     0.70844,     0.70907,     0.70999,     0.71091,     0.71133,     0.71161,      0.7119,     0.71218,     0.71246,\n            0.71274,     0.71302,      0.7133,     0.71571,     0.71657,     0.71742,     0.71845,     0.71975,     0.71911,     0.71868,     0.71892,     0.71917,     0.71941,     0.71966,      0.7199,     0.72015,     0.72039,     0.72064,     0.72104,     0.72164,     0.72224,     0.72284,     0.72377,\n            0.72498,     0.72591,     0.72665,     0.72739,     0.72837,     0.72982,     0.73158,     0.73395,     0.73774,     0.73806,     0.73838,     0.73869,     0.73901,     0.73933,     0.73965,     0.73996,     0.74411,      0.7478,     0.74841,     0.74902,     0.74963,     0.75012,     0.75042,\n            0.75072,     0.75103,     0.75133,     0.75163,     0.75193,     0.75223,     0.75254,     0.75357,     0.75462,     0.75605,     0.75788,     0.76046,     0.76224,     0.76307,     0.76335,     0.76364,     0.76392,      0.7642,     0.76449,     0.76477,     0.76506,     0.76534,     0.76855,\n            0.76959,     0.77063,     0.77098,     0.77116,     0.77134,     0.77152,      0.7717,     0.77188,     0.77206,     0.77224,     0.77243,     0.77261,     0.77279,     0.77297,     0.77315,     0.77333,     0.77351,     0.77375,     0.77399,     0.77423,     0.77448,     0.77472,     0.77497,\n            0.77521,     0.77545,      0.7757,     0.77594,     0.77618,     0.77809,     0.77935,     0.78002,     0.78068,     0.78134,     0.78183,     0.78212,     0.78242,     0.78271,     0.78301,      0.7833,     0.78359,     0.78389,     0.78418,     0.78454,     0.78568,     0.78682,     0.79058,\n            0.79144,      0.7923,     0.79294,     0.79318,     0.79342,     0.79366,      0.7939,     0.79414,     0.79438,     0.79462,     0.79486,      0.7951,     0.79534,     0.79558,     0.79455,     0.79233,     0.79184,     0.79268,     0.79352,     0.79455,      0.7967,     0.79734,     0.79764,\n            0.79794,     0.79824,     0.79853,     0.79883,     0.79913,     0.79942,     0.79972,     0.80001,     0.80025,     0.80049,     0.80073,     0.80097,     0.80121,     0.80145,     0.80169,     0.80193,     0.80217,     0.80241,     0.80265,     0.80289,     0.80375,     0.80472,     0.80568,\n             0.8075,     0.80903,     0.80961,     0.81019,     0.81078,     0.81136,     0.81187,     0.81213,     0.81239,     0.81266,     0.81292,     0.81318,     0.81344,     0.81371,     0.81397,     0.81423,     0.81449,     0.81475,     0.81514,     0.81557,       0.816,     0.81643,     0.81686,\n            0.81728,     0.81771,     0.81793,     0.81805,     0.81817,     0.81828,      0.8184,     0.81852,     0.81864,     0.81876,     0.81888,       0.819,     0.81912,     0.81924,     0.81936,     0.81948,      0.8196,     0.81972,     0.81983,     0.81995,     0.82007,     0.82019,     0.82031,\n            0.82043,     0.82055,     0.82067,     0.82079,     0.82091,     0.82113,     0.82135,     0.82157,     0.82178,       0.822,     0.82222,     0.82244,     0.82265,     0.82287,     0.82309,     0.82331,     0.82352,     0.82374,     0.82396,     0.82415,     0.82434,     0.82453,     0.82472,\n            0.82491,      0.8251,     0.82529,     0.82548,     0.82567,     0.82586,     0.82604,     0.82623,     0.82642,     0.82661,      0.8268,     0.82699,     0.82815,     0.82998,     0.83305,     0.83349,     0.83366,     0.83383,       0.834,     0.83417,     0.83435,     0.83452,     0.83469,\n            0.83486,     0.83503,      0.8352,     0.83537,     0.83554,     0.83571,     0.83588,     0.83605,     0.83622,      0.8364,     0.83673,     0.83732,     0.83792,     0.83851,     0.83911,      0.8397,     0.84021,     0.84072,     0.84122,     0.84173,     0.84224,     0.84274,     0.84304,\n            0.84322,     0.84341,      0.8436,     0.84379,     0.84398,     0.84416,     0.84435,     0.84454,     0.84472,     0.84491,      0.8451,     0.84529,     0.84547,     0.84566,     0.84585,     0.84603,     0.84733,     0.84988,     0.85111,     0.85234,     0.85507,     0.85655,     0.85727,\n            0.85799,     0.85871,      0.8594,     0.85973,     0.86006,     0.86039,     0.86072,     0.86105,     0.86138,     0.86171,     0.86204,     0.86236,     0.86269,     0.86293,     0.86316,     0.86338,     0.86361,     0.86383,     0.86406,     0.86428,     0.86451,     0.86473,     0.86496,\n            0.86518,     0.86541,     0.86563,     0.86585,     0.86608,     0.86647,     0.86692,     0.86737,     0.86783,     0.86828,     0.86873,     0.86918,      0.8696,     0.86981,     0.87002,     0.87023,     0.87045,     0.87066,     0.87087,     0.87108,     0.87129,      0.8715,     0.87171,\n            0.87192,     0.87213,     0.87235,     0.87256,     0.87277,     0.87298,     0.87326,     0.87356,     0.87385,     0.87415,     0.87445,     0.87475,     0.87504,     0.87534,     0.87564,     0.87593,     0.87623,     0.87648,     0.87639,     0.87629,     0.87619,     0.87609,     0.87599,\n            0.87589,      0.8758,      0.8757,      0.8756,      0.8755,      0.8754,      0.8753,      0.8752,     0.87511,     0.87501,     0.87491,     0.87481,     0.87471,     0.87461,     0.87452,     0.87442,     0.87432,     0.87422,     0.87412,     0.87402,     0.87392,     0.87383,     0.87373,\n            0.87363,     0.87353,     0.87343,     0.87333,     0.87323,     0.87314,     0.87304,     0.87294,     0.87284,     0.87274,     0.87264,     0.87254,     0.87244,     0.87235,     0.87225,     0.87215,     0.87205,     0.87171,     0.87113,     0.87055,     0.86997,     0.86939,     0.86881,\n            0.86823,     0.86765,      0.8683,      0.8695,     0.87069,     0.87073,     0.87042,      0.8701,     0.86979,     0.86948,     0.86917,     0.86886,     0.86854,     0.86823,     0.86792,     0.86761,     0.86729,     0.86698,     0.86667,     0.86664,     0.86845,     0.86997,     0.87023,\n            0.87049,     0.87075,     0.87101,     0.87127,     0.87153,     0.87179,     0.87205,     0.87231,     0.87257,     0.87283,     0.87309,     0.87335,     0.87359,     0.87382,     0.87405,     0.87428,     0.87451,     0.87474,     0.87497,      0.8752,     0.87543,     0.87566,     0.87589,\n            0.87612,     0.87634,     0.87657,      0.8768,     0.87703,     0.87802,     0.87908,     0.88013,     0.88443,      0.8847,     0.88497,     0.88524,     0.88551,     0.88578,     0.88605,     0.88632,     0.88658,     0.88685,     0.88712,     0.88739,     0.88766,     0.88792,      0.8885,\n            0.88913,     0.88977,      0.8904,     0.89103,     0.89166,     0.89081,     0.88995,     0.88909,     0.88822,     0.88736,     0.88822,     0.89013,     0.89086,     0.89102,     0.89117,     0.89132,     0.89148,     0.89163,     0.89179,     0.89194,      0.8921,     0.89225,      0.8924,\n            0.89256,     0.89271,     0.89287,     0.89302,     0.89317,     0.89333,     0.89348,     0.89364,     0.89379,     0.89394,      0.8941,     0.89425,      0.8944,     0.89418,     0.89298,     0.89178,     0.89058,     0.88994,     0.89021,     0.89049,     0.89077,     0.89105,     0.89133,\n             0.8916,     0.89188,     0.89216,     0.89243,     0.89271,     0.89299,     0.89326,     0.89354,     0.89425,     0.89512,     0.89599,     0.89686,     0.89773,     0.89861,     0.89949,     0.90036,     0.90124,     0.90105,     0.90081,     0.90056,     0.90031,     0.90006,     0.89981,\n            0.89956,     0.89931,     0.89907,     0.89882,     0.89857,     0.89832,     0.89807,     0.89782,     0.89757,     0.89732,     0.89707,     0.89682,     0.89657,     0.89708,     0.89765,     0.89821,     0.89878,     0.89935,     0.89992,      0.9004,     0.90005,      0.8997,     0.89935,\n            0.89901,     0.89866,     0.89831,     0.89796,     0.89761,     0.89726,      0.8969,     0.89655,      0.8962,     0.89585,     0.89597,      0.8967,     0.89743,     0.89816,     0.89888,     0.89962,     0.90043,     0.90124,     0.90205,     0.90285,     0.90305,     0.90057,     0.90273,\n            0.90303,     0.90334,     0.90364,     0.90395,     0.90425,     0.90456,     0.90486,     0.90517,     0.90547,     0.90578,     0.90608,     0.90638,     0.90662,     0.90596,     0.90529,     0.90463,     0.90396,     0.90329,     0.90262,     0.90195,     0.90256,      0.9036,     0.90464,\n            0.90567,     0.90517,      0.9044,     0.90362,     0.90284,     0.90207,     0.90129,     0.90123,     0.90187,     0.90252,     0.90316,      0.9038,     0.90444,     0.90521,     0.90662,     0.90802,     0.90891,     0.90812,     0.90734,     0.90655,     0.90577,     0.90498,     0.90419,\n            0.90258,     0.90086,     0.89914,     0.89573,     0.88711,     0.88375,     0.88431,     0.88488,     0.88544,     0.88601,     0.88657,     0.88713,      0.8877,     0.88889,     0.89031,     0.89173,      0.8906,     0.88881,     0.88702,     0.89021,     0.88931,     0.88841,     0.88751,\n             0.8866,     0.88565,     0.88201,     0.87983,     0.87883,     0.87783,     0.87682,     0.87582,     0.87555,     0.87844,     0.87653,     0.87342,     0.87201,     0.87061,      0.8692,     0.86784,     0.86658,     0.86532,     0.86405,     0.86278,     0.86174,     0.86069,     0.85965,\n            0.85861,     0.85756,     0.85676,     0.85611,     0.85547,     0.85483,     0.85418,     0.85354,     0.85289,     0.85225,      0.8516,     0.84827,     0.83946,     0.83797,     0.83648,     0.83498,     0.83307,     0.83066,     0.82232,      0.8215,     0.82068,     0.81986,     0.81904,\n            0.81822,      0.8174,     0.81657,     0.81458,      0.8121,      0.8096,     0.80709,     0.80457,     0.79582,     0.79325,     0.79104,     0.78943,     0.78781,     0.78618,     0.77901,     0.77707,     0.77518,     0.77328,     0.76861,     0.76324,     0.75839,     0.75154,     0.74542,\n            0.74263,     0.73983,     0.73762,      0.7356,     0.73358,     0.72285,     0.71692,     0.71327,     0.71044,     0.70881,     0.70717,     0.70552,     0.69648,     0.67394,     0.65841,     0.64317,     0.64209,       0.641,     0.63991,     0.63882,     0.63773,     0.63663,     0.63554,\n            0.62398,     0.61978,     0.60725,     0.60295,     0.59863,     0.59427,     0.58583,     0.58257,      0.5796,     0.57631,      0.5541,     0.54827,     0.54516,     0.54204,     0.53791,     0.53317,     0.53021,     0.52808,     0.52595,     0.52381,     0.51949,     0.51077,     0.50421,\n            0.47933,      0.4742,     0.44075,     0.43536,     0.39652,     0.38891,     0.37297,     0.36828,      0.3629,     0.35696,     0.33793,     0.32739,     0.31061,     0.29791,     0.25272,     0.24078,     0.23391,       0.201,     0.19909,     0.19718,     0.19526,     0.19334,     0.19141,\n            0.18949,     0.18755,     0.18436,     0.18111,     0.17786,     0.17459,     0.16748,     0.15783,     0.15031,     0.12863,     0.12625,     0.12386,     0.12146,     0.11906,     0.11665,     0.11424,     0.10511,    0.092531,    0.081944,    0.080136,    0.078325,     0.07651,    0.074692,\n           0.072871,    0.071046,    0.069218,    0.067386,    0.049492,    0.047962,    0.046431,    0.044897,     0.04336,    0.041821,     0.04028,    0.038736,     0.03719,    0.035641,     0.03409,    0.032472,    0.030841,    0.029208,    0.027573,    0.025934,    0.024293,    0.022649,    0.021003,\n           0.019353,    0.017701,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.084857,    0.084857,     0.13247,     0.17176,     0.19425,     0.21682,     0.23127,     0.24355,     0.26212,     0.27911,     0.29156,     0.29805,     0.31024,     0.32231,     0.32868,     0.33465,     0.34027,     0.35133,     0.36475,     0.37158,      0.3818,     0.38639,     0.39188,\n            0.39408,      0.4004,     0.40567,      0.4119,     0.41523,     0.41913,     0.42586,     0.43222,     0.43616,      0.4399,     0.44252,     0.44967,     0.45207,     0.45404,     0.45511,       0.456,     0.46035,     0.46165,     0.46271,      0.4652,     0.47065,     0.47517,     0.47771,\n            0.47949,     0.48049,      0.4838,     0.48558,     0.49399,     0.49529,     0.49734,     0.49838,     0.49914,     0.49989,     0.50133,     0.50523,      0.5071,     0.50766,     0.50821,     0.50877,     0.50956,     0.51068,     0.51262,     0.51449,     0.51557,     0.52088,     0.52536,\n            0.52652,     0.52742,     0.52852,     0.53256,     0.53554,     0.53952,     0.54181,     0.54269,     0.54357,     0.54742,     0.54981,     0.55742,     0.55811,     0.55879,     0.55947,     0.56026,     0.56141,     0.56256,     0.56309,     0.56344,      0.5638,     0.56415,      0.5645,\n            0.56486,     0.56521,     0.56556,      0.5686,     0.56968,     0.57076,     0.57206,     0.57372,     0.57548,     0.57543,     0.57574,     0.57606,     0.57637,     0.57669,       0.577,     0.57732,     0.57763,     0.57795,     0.57846,     0.57923,     0.58001,     0.58079,     0.58198,\n            0.58355,     0.58476,     0.58572,     0.58667,     0.58796,     0.58985,     0.59215,     0.59526,     0.60026,     0.60068,     0.60111,     0.60153,     0.60195,     0.60237,     0.60279,     0.60321,     0.60875,      0.6137,     0.61452,     0.61535,     0.61617,     0.61683,     0.61724,\n            0.61765,     0.61806,     0.61847,     0.61887,     0.61928,     0.61969,      0.6201,     0.62151,     0.62294,      0.6249,      0.6274,     0.63093,     0.63339,     0.63454,     0.63493,     0.63532,     0.63572,     0.63611,      0.6365,      0.6369,     0.63729,     0.63768,     0.64216,\n            0.64361,     0.64507,     0.64555,     0.64581,     0.64606,     0.64632,     0.64657,     0.64682,     0.64708,     0.64733,     0.64758,     0.64784,     0.64809,     0.64834,      0.6486,     0.64885,     0.64911,     0.64944,     0.64979,     0.65013,     0.65048,     0.65082,     0.65116,\n            0.65151,     0.65185,      0.6522,     0.65254,     0.65288,     0.65558,     0.65738,     0.65833,     0.65927,     0.66021,     0.66091,     0.66133,     0.66175,     0.66218,      0.6626,     0.66302,     0.66344,     0.66386,     0.66428,      0.6648,     0.66644,     0.66808,     0.67352,\n            0.67477,     0.67602,     0.67695,      0.6773,     0.67765,       0.678,     0.67836,     0.67871,     0.67906,     0.67941,     0.67976,     0.68011,     0.68046,     0.68081,     0.68046,     0.67945,     0.67971,     0.68095,     0.68218,     0.68371,      0.6869,     0.68786,      0.6883,\n            0.68875,     0.68919,     0.68963,     0.69008,     0.69052,     0.69096,      0.6914,     0.69184,      0.6922,     0.69256,     0.69292,     0.69328,     0.69364,       0.694,     0.69436,     0.69472,     0.69508,     0.69543,     0.69579,     0.69615,     0.69745,     0.69891,     0.70036,\n            0.70313,     0.70544,     0.70633,     0.70721,      0.7081,     0.70899,     0.70977,     0.71017,     0.71057,     0.71097,     0.71138,     0.71178,     0.71218,     0.71258,     0.71298,     0.71339,     0.71379,     0.71419,     0.71479,     0.71545,     0.71611,     0.71677,     0.71743,\n            0.71809,     0.71875,     0.71908,     0.71927,     0.71945,     0.71964,     0.71982,     0.72001,     0.72019,     0.72037,     0.72056,     0.72074,     0.72093,     0.72111,      0.7213,     0.72148,     0.72167,     0.72185,     0.72204,     0.72222,     0.72241,     0.72259,     0.72278,\n            0.72296,     0.72314,     0.72333,     0.72351,     0.72371,     0.72405,     0.72439,     0.72473,     0.72507,     0.72541,     0.72574,     0.72608,     0.72642,     0.72676,      0.7271,     0.72744,     0.72778,     0.72812,     0.72845,     0.72875,     0.72905,     0.72935,     0.72965,\n            0.72994,     0.73024,     0.73054,     0.73083,     0.73113,     0.73143,     0.73173,     0.73202,     0.73232,     0.73262,     0.73292,     0.73321,     0.73504,     0.73793,     0.74279,     0.74349,     0.74376,     0.74404,     0.74431,     0.74458,     0.74486,     0.74513,      0.7454,\n            0.74567,     0.74595,     0.74622,     0.74649,     0.74676,     0.74704,     0.74731,     0.74758,     0.74786,     0.74813,     0.74866,     0.74961,     0.75057,     0.75152,     0.75248,     0.75343,     0.75425,     0.75507,     0.75589,     0.75671,     0.75753,     0.75835,     0.75882,\n            0.75913,     0.75943,     0.75974,     0.76004,     0.76034,     0.76065,     0.76095,     0.76126,     0.76156,     0.76187,     0.76217,     0.76247,     0.76278,     0.76308,     0.76339,     0.76369,     0.76581,     0.76999,     0.77201,     0.77403,     0.77855,       0.781,     0.78221,\n            0.78341,     0.78461,     0.78576,     0.78631,     0.78686,     0.78742,     0.78797,     0.78852,     0.78907,     0.78962,     0.79017,     0.79073,     0.79128,     0.79169,     0.79206,     0.79244,     0.79282,      0.7932,     0.79358,     0.79396,     0.79434,     0.79472,      0.7951,\n            0.79548,     0.79586,     0.79624,     0.79662,     0.79699,     0.79765,     0.79842,     0.79919,     0.79996,     0.80073,      0.8015,     0.80227,     0.80298,     0.80334,      0.8037,     0.80406,     0.80442,     0.80478,     0.80514,     0.80551,     0.80587,     0.80623,     0.80659,\n            0.80695,     0.80731,     0.80767,     0.80803,      0.8084,     0.80876,     0.80924,     0.80975,     0.81026,     0.81078,     0.81129,      0.8118,     0.81231,     0.81282,     0.81334,     0.81385,     0.81436,     0.81481,     0.81478,     0.81475,     0.81472,     0.81469,     0.81466,\n            0.81463,      0.8146,     0.81457,     0.81454,     0.81451,     0.81448,     0.81445,     0.81442,     0.81439,     0.81436,     0.81433,      0.8143,     0.81427,     0.81424,     0.81421,     0.81418,     0.81414,     0.81411,     0.81408,     0.81405,     0.81402,     0.81399,     0.81396,\n            0.81393,      0.8139,     0.81387,     0.81384,     0.81381,     0.81378,     0.81375,     0.81372,     0.81369,     0.81366,     0.81363,      0.8136,     0.81357,     0.81354,     0.81351,     0.81348,     0.81345,     0.81334,     0.81316,     0.81298,      0.8128,     0.81262,     0.81244,\n            0.81226,     0.81208,     0.81349,     0.81559,      0.8177,     0.81811,     0.81801,     0.81792,     0.81782,     0.81773,     0.81763,     0.81754,     0.81745,     0.81735,     0.81726,     0.81716,     0.81707,     0.81697,     0.81688,     0.81723,     0.82045,     0.82316,     0.82363,\n             0.8241,     0.82456,     0.82503,      0.8255,     0.82597,     0.82643,      0.8269,     0.82737,     0.82783,      0.8283,     0.82877,     0.82924,     0.82968,     0.83009,     0.83051,     0.83092,     0.83134,     0.83175,     0.83217,     0.83258,       0.833,     0.83341,     0.83383,\n            0.83424,     0.83466,     0.83507,     0.83549,      0.8359,      0.8377,     0.83963,     0.84156,     0.84946,     0.84995,     0.85045,     0.85095,     0.85145,     0.85194,     0.85244,     0.85294,     0.85344,     0.85393,     0.85443,     0.85493,     0.85542,     0.85592,     0.85699,\n            0.85817,     0.85935,     0.86053,     0.86171,     0.86289,      0.8627,     0.86249,     0.86228,     0.86207,     0.86187,     0.86405,     0.86767,     0.86905,     0.86934,     0.86964,     0.86993,     0.87023,     0.87052,     0.87082,     0.87111,     0.87141,      0.8717,     0.87199,\n            0.87229,     0.87258,     0.87288,     0.87317,     0.87347,     0.87376,     0.87406,     0.87435,     0.87464,     0.87494,     0.87523,     0.87553,     0.87582,     0.87596,     0.87569,     0.87543,     0.87516,     0.87521,     0.87574,     0.87628,     0.87682,     0.87736,      0.8779,\n            0.87844,     0.87897,     0.87951,     0.88005,     0.88059,     0.88113,     0.88167,      0.8822,     0.88358,     0.88529,     0.88699,     0.88869,     0.89041,     0.89214,     0.89387,      0.8956,     0.89734,     0.89739,     0.89735,      0.8973,     0.89725,     0.89721,     0.89716,\n            0.89711,     0.89707,     0.89702,     0.89697,     0.89693,     0.89688,     0.89683,     0.89679,     0.89674,     0.89669,     0.89665,      0.8966,     0.89656,      0.8976,     0.89874,     0.89988,     0.90103,     0.90217,     0.90331,     0.90434,     0.90428,     0.90422,     0.90416,\n             0.9041,     0.90404,     0.90397,     0.90391,     0.90385,     0.90379,     0.90373,     0.90367,     0.90361,     0.90354,     0.90415,     0.90564,     0.90713,     0.90862,     0.91011,     0.91161,     0.91328,     0.91495,     0.91662,     0.91829,     0.91957,      0.9192,     0.92743,\n            0.92807,     0.92872,     0.92936,     0.93001,     0.93065,      0.9313,     0.93195,     0.93259,     0.93324,     0.93388,     0.93453,     0.93518,     0.93577,     0.93569,     0.93561,     0.93553,     0.93545,     0.93537,     0.93529,     0.93521,     0.93686,      0.9391,     0.94134,\n            0.94358,     0.94385,     0.94377,     0.94369,      0.9436,     0.94352,     0.94344,     0.94411,     0.94553,     0.94695,     0.94836,     0.94978,      0.9512,     0.95289,     0.95602,     0.95915,     0.96152,     0.96147,     0.96141,     0.96135,     0.96129,     0.96123,     0.96117,\n            0.96105,     0.96092,     0.96079,     0.96053,     0.95986,     0.95965,     0.96099,     0.96233,     0.96367,     0.96501,     0.96634,     0.96768,     0.96902,     0.97187,     0.97528,     0.97869,     0.97932,     0.97925,     0.97918,     0.98946,     0.98944,     0.98942,      0.9894,\n            0.98938,     0.98936,     0.98928,     0.98924,     0.98921,     0.98919,     0.98917,     0.98915,     0.99054,     0.99797,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,     0.99138,     0.99138,     0.99138,     0.99138,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.97774,\n            0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,     0.96833,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,\n            0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,\n            0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,\n            0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.95828,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,     0.95461,     0.95019,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94826,     0.94807,     0.94788,     0.94769,      0.9475,     0.94731,\n            0.94712,     0.94693,     0.94674,     0.94655,     0.94637,     0.94618,     0.94599,      0.9458,     0.94561,     0.94542,     0.94523,     0.94504,     0.94485,     0.94466,     0.94448,     0.94429,      0.9441,     0.94391,     0.94372,     0.94353,     0.94334,     0.94315,     0.94296,\n            0.94277,     0.94258,      0.9424,     0.94221,     0.94202,     0.94183,     0.94164,     0.94145,     0.94126,     0.94107,     0.94088,     0.94069,     0.94051,     0.94032,     0.94013,     0.93994,     0.93975,      0.9391,       0.938,     0.93689,     0.93579,     0.93468,     0.93358,\n            0.93248,     0.93137,     0.93103,     0.93103,     0.93103,     0.93058,     0.92999,      0.9294,     0.92881,     0.92823,     0.92764,     0.92705,     0.92646,     0.92587,     0.92528,     0.92469,      0.9241,     0.92351,     0.92292,     0.92241,     0.92241,     0.92241,     0.92241,\n            0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,\n            0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,\n            0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92082,     0.91922,     0.91761,       0.916,      0.9144,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,\n            0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91317,     0.91096,     0.90875,     0.90654,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,\n            0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90475,     0.90429,     0.90384,     0.90339,     0.90293,     0.90248,\n            0.90203,     0.90157,     0.90112,     0.90067,     0.90021,     0.89976,     0.89931,     0.89885,      0.8984,     0.89795,     0.89749,     0.89704,     0.89659,     0.89655,     0.89655,     0.89655,     0.89655,     0.89655,     0.89655,      0.8965,     0.89586,     0.89523,      0.8946,\n            0.89397,     0.89334,     0.89271,     0.89208,     0.89145,     0.89081,     0.89018,     0.88955,     0.88892,     0.88829,     0.88793,     0.88793,     0.88793,     0.88793,     0.88793,     0.88793,     0.88793,     0.88793,     0.88793,     0.88793,      0.8871,     0.88268,     0.87931,\n            0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87923,     0.87805,     0.87688,      0.8757,     0.87452,     0.87334,     0.87216,     0.87099,     0.87069,     0.87069,     0.87069,\n            0.87069,     0.86954,     0.86818,     0.86682,     0.86546,      0.8641,     0.86274,     0.86207,     0.86207,     0.86207,     0.86207,     0.86207,     0.86207,     0.86207,     0.86207,     0.86207,     0.86175,     0.86039,     0.85903,     0.85767,     0.85631,     0.85495,     0.85359,\n            0.85081,     0.84786,     0.84492,     0.83912,     0.82461,     0.81897,     0.81897,     0.81897,     0.81897,     0.81897,     0.81897,     0.81897,     0.81897,     0.81897,     0.81897,     0.81897,     0.81661,     0.81367,     0.81072,     0.80906,     0.80759,     0.80612,     0.80464,\n            0.80317,     0.80161,     0.79572,     0.79221,      0.7906,       0.789,     0.78739,     0.78579,     0.78448,     0.78448,     0.78021,     0.77528,     0.77307,     0.77086,     0.76865,     0.76653,     0.76457,     0.76261,     0.76064,     0.75868,     0.75706,     0.75546,     0.75385,\n            0.75224,     0.75064,     0.74941,     0.74842,     0.74744,     0.74646,     0.74548,      0.7445,     0.74352,     0.74253,     0.74155,     0.73652,     0.72334,     0.72113,     0.71892,     0.71671,      0.7139,     0.71036,     0.69825,     0.69707,      0.6959,     0.69472,     0.69354,\n            0.69236,     0.69118,     0.69001,     0.68717,     0.68364,      0.6801,     0.67657,     0.67303,     0.66088,     0.65734,     0.65432,     0.65211,      0.6499,     0.64769,     0.63801,     0.63542,     0.63289,     0.63037,     0.62418,     0.61712,     0.61081,     0.60198,     0.59415,\n            0.59062,     0.58708,     0.58431,     0.58178,     0.57926,     0.56598,     0.55875,     0.55433,     0.55092,     0.54895,     0.54699,     0.54503,      0.5343,     0.50822,     0.49077,     0.47403,     0.47285,     0.47167,     0.47049,     0.46931,     0.46814,     0.46696,     0.46578,\n            0.45346,     0.44905,     0.43601,     0.43159,     0.42717,     0.42275,     0.41425,       0.411,     0.40806,      0.4048,     0.38322,     0.37767,     0.37472,     0.37178,      0.3679,     0.36348,     0.36073,     0.35877,     0.35681,     0.35484,     0.35089,     0.34298,     0.33709,\n            0.31521,     0.31079,     0.28266,     0.27825,     0.24729,      0.2414,     0.22923,      0.2257,     0.22167,     0.21725,     0.20332,     0.19574,     0.18386,     0.17502,     0.14464,     0.13687,     0.13245,     0.11173,     0.11055,     0.10937,     0.10819,     0.10701,     0.10584,\n            0.10466,     0.10348,     0.10154,    0.099574,     0.09761,    0.095646,    0.091393,    0.085678,     0.08126,    0.068737,    0.067377,    0.066018,    0.064658,    0.063299,    0.061939,     0.06058,    0.055472,     0.04851,    0.042722,     0.04174,    0.040759,    0.039777,    0.038795,\n           0.037813,    0.036831,     0.03585,    0.034868,    0.025374,     0.02457,    0.023767,    0.022964,     0.02216,    0.021357,    0.020554,    0.019751,    0.018947,    0.018144,    0.017341,    0.016504,    0.015662,    0.014821,    0.013979,    0.013138,    0.012296,    0.011454,    0.010613,\n          0.0097713,   0.0089297,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.6965791677806015\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.66769])\nnames: {0: 'Defect'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9195732895057219, 'metrics/recall(B)': 0.8871026198612404, 'metrics/mAP50(B)': 0.9565508612309526, 'metrics/mAP50-95(B)': 0.6676934240638958, 'fitness': 0.6965791677806015}\nsave_dir: PosixPath('runs/detect/train')\nspeed: {'preprocess': 0.2489348222196794, 'inference': 24.23836207222444, 'loss': 0.00021693333312416345, 'postprocess': 2.961141055554132}\ntask: 'detect'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "IoOL1tmt11Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your trained model (change this to the correct path)\n",
        "model_path = '/kaggle/working/yolov5/runs/detect/train/weights/best.pt'  # Replace with the correct path to the best model\n",
        "# Evaluate the model\n",
        "model = YOLO(model_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T11:45:32.916107Z",
          "iopub.execute_input": "2025-04-14T11:45:32.916379Z",
          "iopub.status.idle": "2025-04-14T11:45:33.011737Z",
          "shell.execute_reply.started": "2025-04-14T11:45:32.916363Z",
          "shell.execute_reply": "2025-04-14T11:45:33.011143Z"
        },
        "id": "wOIWdrb0SXtg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the validation dataset (it automatically uses the val split from your YAML file)\n",
        "results = model.val(data=data_yaml_path)"
      ],
      "metadata": {
        "id": "UpmF6OQv13O0",
        "trusted": true,
        "outputId": "2c761a17-306a-4eb9-aa3d-9d272c4ebdcf",
        "execution": {
          "iopub.status.busy": "2025-04-14T09:47:47.008644Z",
          "iopub.execute_input": "2025-04-14T09:47:47.009132Z",
          "iopub.status.idle": "2025-04-14T09:48:02.916010Z",
          "shell.execute_reply.started": "2025-04-14T09:47:47.009057Z",
          "shell.execute_reply": "2025-04-14T09:48:02.915221Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.107 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nYOLOv5s summary (fused): 84 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 732.71it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:10<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.931      0.948      0.964      0.627\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 3.1ms preprocess, 7.4ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Original paths\n",
        "image_dir = \"/kaggle/input/shoe-data/Data-defect/Data-defect/test-def/images\"\n",
        "label_dir = \"/kaggle/input/shoe-data/Data-defect/Data-defect/test-def/labels\"\n",
        "\n",
        "# Output base\n",
        "base_output = \"/kaggle/working/split_image\"\n",
        "\n",
        "# Output folders\n",
        "defect_img = os.path.join(base_output, \"defect/images\")\n",
        "defect_lbl = os.path.join(base_output, \"defect/labels\")\n",
        "no_defect_img = os.path.join(base_output, \"no_defect/images\")\n",
        "no_defect_lbl = os.path.join(base_output, \"no_defect/labels\")\n",
        "\n",
        "# Create dirs\n",
        "os.makedirs(defect_img, exist_ok=True)\n",
        "os.makedirs(defect_lbl, exist_ok=True)\n",
        "os.makedirs(no_defect_img, exist_ok=True)\n",
        "os.makedirs(no_defect_lbl, exist_ok=True)\n",
        "\n",
        "# Loop over all images\n",
        "for file in os.listdir(image_dir):\n",
        "    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
        "        name = os.path.splitext(file)[0]\n",
        "        img_path = os.path.join(image_dir, file)\n",
        "        lbl_path = os.path.join(label_dir, f\"{name}.txt\")\n",
        "\n",
        "        # Check if label file exists and is empty or not\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            # Defect image\n",
        "            shutil.copy(img_path, os.path.join(defect_img, file))\n",
        "            shutil.copy(lbl_path, os.path.join(defect_lbl, f\"{name}.txt\"))\n",
        "        else:\n",
        "            # No-defect image\n",
        "            shutil.copy(img_path, os.path.join(no_defect_img, file))\n",
        "            # Create an empty label file\n",
        "            open(os.path.join(no_defect_lbl, f\"{name}.txt\"), 'w').close()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T11:45:33.013121Z",
          "iopub.execute_input": "2025-04-14T11:45:33.013402Z",
          "iopub.status.idle": "2025-04-14T11:45:39.792354Z",
          "shell.execute_reply.started": "2025-04-14T11:45:33.013375Z",
          "shell.execute_reply": "2025-04-14T11:45:39.791597Z"
        },
        "id": "VduQWyZxSXti"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Create the YAMLs\n",
        "def write_yaml(path, yaml_content):\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(yaml_content)\n",
        "\n",
        "defect_yaml = \"\"\"\\\n",
        "train: /kaggle/working/split_image/defect/images\n",
        "val: /kaggle/working/split_image/defect/images\n",
        "test: /kaggle/working/split_image/defect/images\n",
        "nc: 1\n",
        "names: ['Defect']\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "no_defect_yaml = \"\"\"\\\n",
        "path: /kaggle/working/split_image/no_defect\n",
        "train: images\n",
        "val: images\n",
        "test: images\n",
        "\n",
        "names:\n",
        "  0: 'No Defect'\n",
        "\"\"\"\n",
        "\n",
        "write_yaml(\"/kaggle/working/data_defect.yaml\", defect_yaml)\n",
        "write_yaml(\"/kaggle/working/data_no_defect.yaml\", no_defect_yaml)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T11:55:58.648355Z",
          "iopub.execute_input": "2025-04-14T11:55:58.648697Z",
          "iopub.status.idle": "2025-04-14T11:55:58.654223Z",
          "shell.execute_reply.started": "2025-04-14T11:55:58.648667Z",
          "shell.execute_reply": "2025-04-14T11:55:58.653618Z"
        },
        "id": "Z_kM2VLpSXtj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write YAML files\n",
        "def write_yaml(path, yaml_content):\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(yaml_content)\n",
        "\n",
        "write_yaml(\"/kaggle/working/data_defect.yaml\", defect_yaml)\n",
        "write_yaml(\"/kaggle/working/data_no_defect.yaml\", no_defect_yaml)\n",
        " # Update with your model path\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(name, yaml_path):\n",
        "    print(f\"\\nğŸ” Evaluating on {name} set...\")\n",
        "    metrics = model.val(data=yaml_path, split=\"test\", save_json=False, save_txt=False, verbose=False)\n",
        "\n",
        "    p = metrics.box.p.mean().item()\n",
        "    r = metrics.box.r.mean().item()\n",
        "    map50 = metrics.box.map50.item()\n",
        "    map95 = metrics.box.map.item()\n",
        "    f1 = (2 * p * r) / (p + r + 1e-6)\n",
        "\n",
        "    return {\n",
        "        \"precision\": round(p, 4),\n",
        "        \"recall\": round(r, 4),\n",
        "        \"f1-score\": round(f1, 4),\n",
        "        \"accuracy (mAP@50)\": round(map50, 4),\n",
        "        \"mAP@50-95\": round(map95, 4)\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "metrics_dict = {\n",
        "    \"Defect\": evaluate(\"Defect\", \"/kaggle/working/data_defect.yaml\"),\n",
        "}\n",
        "\n",
        "# Print and save results\n",
        "print(\"\\n Final Evaluation Metrics:\")\n",
        "print(json.dumps(metrics_dict, indent=4))\n",
        "\n",
        "with open(\"/kaggle/working/final_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics_dict, f, indent=4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T11:56:00.174441Z",
          "iopub.execute_input": "2025-04-14T11:56:00.174742Z",
          "iopub.status.idle": "2025-04-14T11:56:10.417236Z",
          "shell.execute_reply.started": "2025-04-14T11:56:00.174713Z",
          "shell.execute_reply": "2025-04-14T11:56:10.416542Z"
        },
        "id": "bsqOtmC9SXtj",
        "outputId": "ededfd4f-0209-4616-f8d6-fe1fdbe0059c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nğŸ” Evaluating on Defect set...\nUltralytics 8.3.107 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/split_image/defect/labels.cache... 92 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<?, ?it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all         92        131      0.953      0.933      0.969       0.65\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 6.6ms preprocess, 7.0ms inference, 0.0ms loss, 1.6ms postprocess per image\nResults saved to \u001b[1mruns/detect/val8\u001b[0m\n\n Final Evaluation Metrics:\n{\n    \"Defect\": {\n        \"precision\": 0.9532,\n        \"recall\": 0.9329,\n        \"f1-score\": 0.9429,\n        \"accuracy (mAP@50)\": 0.9692,\n        \"mAP@50-95\": 0.6502\n    }\n}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XCMG71R2SXtl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T12:04:08.990415Z",
          "iopub.execute_input": "2025-04-14T12:04:08.991329Z",
          "iopub.status.idle": "2025-04-14T12:04:09.001532Z",
          "shell.execute_reply.started": "2025-04-14T12:04:08.991291Z",
          "shell.execute_reply": "2025-04-14T12:04:09.000892Z"
        },
        "id": "DbVZg48xSXtl",
        "outputId": "4f1e8daf-7f78-4ef4-9d62-6c9e93036e68"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "YOLO(\n  (model): DetectionModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (2): C3(\n        (cv1): Conv(\n          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (4): C3(\n        (cv1): Conv(\n          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n          (1): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (6): C3(\n        (cv1): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n          (1): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n          (2): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (8): C3(\n        (cv1): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): Conv(\n        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (11): Upsample(scale_factor=2.0, mode='nearest')\n      (12): Concat()\n      (13): C3(\n        (cv1): Conv(\n          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (14): Conv(\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (15): Upsample(scale_factor=2.0, mode='nearest')\n      (16): Concat()\n      (17): C3(\n        (cv1): Conv(\n          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (18): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (19): Concat()\n      (20): C3(\n        (cv1): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (21): Conv(\n        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (act): SiLU(inplace=True)\n      )\n      (22): Concat()\n      (23): C3(\n        (cv1): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (cv3): Conv(\n          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (24): Detect(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.val(data=\"data_defect.yaml\", split=\"test\", conf=0.25)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T12:00:27.608350Z",
          "iopub.execute_input": "2025-04-14T12:00:27.608944Z",
          "iopub.status.idle": "2025-04-14T12:00:37.654713Z",
          "shell.execute_reply.started": "2025-04-14T12:00:27.608899Z",
          "shell.execute_reply": "2025-04-14T12:00:37.653861Z"
        },
        "id": "CI2OQoyISXtm",
        "outputId": "3060fe39-849b-4bfd-cffd-6600293dbab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.107 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/split_image/defect/labels.cache... 92 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<?, ?it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.02s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all         92        131      0.953      0.933      0.968      0.678\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 6.2ms preprocess, 6.9ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mruns/detect/val9\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "ultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([0])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x79875cf58dd0>\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,\n             0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,      0.9899,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,\n            0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,\n            0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,\n            0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,\n            0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,\n            0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,\n            0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.98319,     0.97581,     0.97581,     0.97581,     0.97581,\n            0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,     0.97581,\n            0.97581,     0.97581,     0.97581,     0.96825,     0.96825,     0.96825,     0.96825,     0.96825,     0.96825,     0.96825,     0.96825,     0.95349,     0.95349,     0.95349,     0.95349,     0.95349,     0.95349,     0.95349,     0.94656,     0.94656,     0.94656,     0.94656,     0.94656,\n            0.94656,     0.94656,     0.94656,     0.93985,     0.93985,     0.93985,     0.93985,     0.93985,     0.93985,     0.93985,     0.93985,     0.92647,     0.92647,     0.92647,     0.92647,     0.92647,     0.92647,     0.92647,     0.89694,     0.87333,     0.84973,     0.82613,     0.80252,\n            0.77892,     0.75532,     0.73171,     0.70811,      0.6845,      0.6609,      0.6373,     0.61369,     0.59009,     0.56649,     0.54288,     0.51928,     0.49568,     0.47207,     0.44847,     0.42486,     0.40126,     0.37766,     0.35405,     0.33045,     0.30685,     0.28324,     0.25964,\n            0.23604,     0.21243,     0.18883,     0.16523,     0.14162,     0.11802,    0.094414,    0.070811,    0.047207,    0.023604,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,\n            0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.92989,     0.93133,     0.93288,     0.93341,     0.93353,     0.93365,     0.93376,     0.93388,       0.934,     0.93411,     0.93423,     0.93434,     0.93446,     0.93458,\n            0.93469,     0.93481,     0.93492,     0.93504,     0.93516,     0.93527,     0.93539,      0.9355,     0.93562,     0.93573,     0.93585,     0.93597,     0.93608,      0.9362,     0.93631,     0.93643,     0.93654,     0.93666,     0.93677,     0.93687,     0.93696,     0.93704,     0.93713,\n            0.93722,     0.93731,     0.93739,     0.93748,     0.93757,     0.93766,     0.93775,     0.93783,     0.93792,     0.93801,      0.9381,     0.93818,     0.93827,     0.93836,     0.93845,     0.93853,     0.93862,     0.93871,      0.9388,     0.93888,     0.93897,     0.93906,     0.93915,\n            0.93923,     0.93932,     0.93941,     0.93949,     0.93958,     0.93967,     0.93976,     0.93984,     0.93993,     0.94002,     0.94011,     0.94019,     0.94028,     0.94093,     0.94174,     0.94254,     0.94334,     0.94372,     0.94347,     0.94321,     0.94296,     0.94271,     0.94245,\n             0.9422,     0.94194,     0.94169,     0.94144,     0.94118,     0.94093,     0.94067,     0.94042,     0.94017,     0.93991,     0.94005,      0.9403,     0.94056,     0.94082,     0.94108,     0.94134,      0.9416,     0.94185,     0.94211,     0.94237,     0.94263,     0.94288,     0.94314,\n             0.9434,     0.94351,     0.94362,     0.94373,     0.94384,     0.94395,     0.94406,     0.94418,     0.94429,      0.9444,     0.94451,     0.94462,     0.94473,     0.94484,     0.94495,     0.94506,     0.94518,     0.94529,      0.9454,     0.94551,     0.94562,     0.94573,     0.94584,\n            0.94595,     0.94606,     0.94617,     0.94628,     0.94639,      0.9465,     0.94661,     0.94673,     0.94684,     0.94695,     0.94682,     0.94664,     0.94645,     0.94627,     0.94608,      0.9459,     0.94571,     0.94552,     0.94534,     0.94515,     0.94497,     0.94478,     0.94459,\n            0.94441,     0.94422,     0.94404,     0.94385,     0.94366,     0.94348,     0.94329,      0.9431,     0.94314,      0.9438,     0.94446,     0.94511,     0.94577,     0.94643,     0.94636,     0.94609,     0.94583,     0.94556,      0.9453,     0.94503,     0.94476,      0.9445,     0.94423,\n            0.94397,      0.9437,     0.94343,     0.94317,      0.9429,     0.94263,     0.94269,     0.94295,     0.94321,     0.94347,     0.94373,     0.94399,     0.94425,     0.94451,     0.94477,     0.94503,     0.94529,     0.94555,     0.94581,     0.94607,     0.94605,     0.94588,     0.94572,\n            0.94556,     0.94539,     0.94523,     0.94507,     0.94491,     0.94474,     0.94458,     0.94442,     0.94425,     0.94409,     0.94393,     0.94376,      0.9436,     0.94344,     0.94327,     0.94311,     0.94295,     0.94278,     0.94262,     0.94245,     0.94229,     0.94213,     0.94278,\n            0.94371,     0.94463,     0.94556,     0.94618,     0.94672,     0.94726,     0.94781,     0.94835,     0.94889,      0.9494,     0.94895,      0.9485,     0.94804,     0.94759,     0.94713,     0.94668,     0.94622,     0.94576,     0.94532,     0.94555,     0.94578,     0.94601,     0.94624,\n            0.94647,      0.9467,     0.94693,     0.94717,      0.9474,     0.94763,     0.94786,     0.94809,     0.94832,     0.94855,     0.94878,     0.94901,     0.94866,     0.94828,      0.9479,     0.94751,     0.94713,     0.94675,     0.94636,     0.94598,      0.9456,     0.94521,     0.94483,\n            0.94446,     0.94408,      0.9437,     0.94333,     0.94295,     0.94258,      0.9422,     0.94182,     0.94144,     0.94107,     0.94069,     0.94032,     0.93996,     0.93959,     0.93922,     0.93885,     0.93848,     0.93812,     0.93775,     0.93738,     0.93701,     0.93664,      0.9363,\n            0.93597,     0.93564,     0.93531,     0.93498,     0.93466,     0.93433,       0.934,     0.93367,     0.93334,     0.93301,     0.93268,     0.93235,     0.93268,     0.93322,     0.93375,     0.93429,     0.93482,     0.93536,     0.93589,     0.93558,     0.93505,     0.93452,     0.93399,\n            0.93346,     0.93292,     0.93239,     0.93186,     0.93158,     0.93139,     0.93119,       0.931,     0.93081,     0.93061,     0.93042,     0.93023,     0.93003,     0.92984,     0.92965,     0.92945,     0.92926,     0.92906,     0.92887,     0.92868,     0.92848,     0.92829,     0.92809,\n             0.9279,      0.9277,     0.92751,     0.92673,     0.92545,     0.92416,     0.92298,     0.92235,     0.92172,      0.9211,     0.92047,     0.91984,     0.91921,      0.9144,     0.91394,     0.91359,     0.91324,     0.91289,     0.91254,     0.91219,     0.91184,     0.91149,     0.91114,\n            0.91079,     0.91043,     0.91008,     0.90945,     0.90816,     0.90686,     0.90556,     0.90496,     0.90449,     0.90402,     0.90355,     0.90308,     0.90261,     0.90214,     0.90167,      0.9012,     0.90078,     0.90055,     0.90032,     0.90009,     0.89986,     0.89963,      0.8994,\n            0.89917,     0.89893,      0.8987,     0.89847,     0.89824,     0.89801,     0.89778,     0.89755,     0.89732,     0.89708,     0.89685,     0.89662,     0.89639,     0.89535,      0.8934,     0.89139,     0.88903,     0.88209,     0.88041,     0.87872,     0.87735,     0.87656,     0.87576,\n            0.87497,     0.87417,     0.87337,     0.87228,     0.87073,     0.86918,     0.86774,     0.86656,     0.86538,      0.8642,     0.86309,      0.8623,     0.86151,     0.86072,     0.85993,     0.85913,     0.85825,     0.85525,     0.84853,     0.84886,     0.84918,      0.8495,     0.84983,\n            0.85015,     0.85047,     0.85079,     0.85112,     0.85144,     0.85176,     0.85208,     0.85197,     0.85168,     0.85138,     0.85109,      0.8508,      0.8505,     0.85021,     0.84992,     0.84962,     0.84933,     0.84904,     0.84874,     0.84845,     0.84815,     0.84786,     0.84756,\n            0.84727,     0.84639,      0.8452,     0.84399,     0.84279,     0.84167,     0.84066,     0.83965,     0.83864,     0.83762,     0.83639,     0.83482,     0.83324,     0.83161,      0.8296,     0.82759,     0.82243,      0.8198,     0.81793,      0.8161,     0.81522,     0.81434,     0.81346,\n            0.81258,     0.81169,     0.81081,      0.8104,        0.81,     0.80959,     0.80918,     0.80878,     0.80837,     0.80796,     0.80756,     0.80715,     0.80674,     0.80633,     0.80592,     0.80552,     0.80491,     0.80426,     0.80361,     0.80295,      0.8023,     0.80164,     0.80098,\n            0.80033,     0.79931,     0.79793,     0.79654,     0.79515,     0.78744,     0.78415,     0.77413,     0.76924,     0.76051,      0.7581,     0.75569,     0.74593,     0.73639,     0.73419,     0.73198,     0.72996,     0.72818,     0.72638,     0.72459,     0.72312,     0.72165,     0.72017,\n            0.71869,     0.71269,     0.70406,     0.70108,     0.68654,     0.67928,     0.67197,     0.66477,      0.6615,     0.65288,     0.64995,     0.64701,     0.64543,      0.6444,     0.64338,     0.64235,     0.64132,      0.6403,     0.63926,     0.63738,     0.63542,     0.63345,     0.63102,\n            0.62768,     0.62409,     0.61954,     0.61529,     0.61122,     0.60745,     0.60373,     0.60221,     0.60107,     0.59993,     0.59879,     0.59765,      0.5965,     0.57291,     0.56977,     0.56662,     0.55663,     0.55265,     0.54861,     0.54441,     0.53953,     0.53305,     0.52733,\n             0.5213,     0.51261,     0.50571,     0.50186,     0.49764,     0.49309,     0.48934,     0.48605,     0.48273,     0.47672,     0.46809,     0.46346,     0.46116,     0.45887,     0.45656,      0.4451,     0.44039,     0.41925,      0.4156,     0.41201,     0.39861,     0.39451,     0.39038,\n            0.38572,     0.38102,     0.36337,     0.36173,     0.36126,     0.35937,     0.35748,     0.35558,     0.35368,     0.34113,     0.33589,     0.33052,     0.32433,     0.31779,     0.31072,      0.3012,     0.27807,     0.26173,     0.24942,     0.24108,     0.23936,     0.23764,     0.23592,\n            0.23419,     0.23247,     0.23073,     0.22565,     0.21599,     0.20653,     0.19254,     0.18832,     0.18407,     0.17924,     0.17168,     0.16648,     0.16373,     0.16097,      0.1582,     0.15543,     0.15406,     0.15299,     0.15193,     0.15086,      0.1498,     0.14873,     0.14766,\n            0.14659,     0.14552,     0.14445,     0.14337,      0.1423,      0.1313,     0.11328,     0.11042,     0.10755,     0.10467,     0.10179,     0.10076,    0.099977,    0.099193,     0.09841,    0.097625,     0.09684,    0.096054,    0.095268,    0.094481,    0.093693,    0.092905,    0.092115,\n           0.091326,    0.090535,    0.089744,    0.088953,     0.08816,    0.087383,    0.086645,    0.085905,    0.085166,    0.084426,    0.083685,    0.082943,    0.082201,    0.081459,    0.080716,    0.079972,    0.079228,    0.078483,    0.077738,    0.076992,    0.076245,    0.075498,     0.07475,\n           0.074002,    0.072654,    0.070277,    0.067895,    0.065506,    0.063112,    0.060712,    0.043276,    0.034671,    0.029757,     0.02908,    0.028402,    0.027724,    0.027045,    0.026366,    0.025686,    0.025006,    0.024326,    0.023645,    0.022963,    0.022281,    0.021598,    0.020915,\n           0.020232,    0.019548,    0.018864,    0.018179,    0.017494,    0.016808,    0.016122,    0.015435,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[        0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,     0.90271,     0.90561,     0.90663,     0.90685,     0.90707,     0.90729,     0.90751,     0.90773,     0.90794,     0.90816,     0.90838,      0.9086,     0.90882,\n            0.90904,     0.90926,     0.90948,      0.9097,     0.90992,     0.91014,     0.91036,     0.91058,      0.9108,     0.91101,     0.91123,     0.91145,     0.91167,     0.91189,     0.91211,     0.91233,     0.91255,     0.91277,     0.91299,     0.91317,     0.91333,      0.9135,     0.91367,\n            0.91384,       0.914,     0.91417,     0.91434,      0.9145,     0.91467,     0.91484,       0.915,     0.91517,     0.91534,      0.9155,     0.91567,     0.91584,       0.916,     0.91617,     0.91634,      0.9165,     0.91667,     0.91684,       0.917,     0.91717,     0.91734,      0.9175,\n            0.91767,     0.91784,       0.918,     0.91817,     0.91834,      0.9185,     0.91867,     0.91884,     0.91901,     0.91917,     0.91934,     0.91951,     0.91967,     0.92092,     0.92246,       0.924,     0.92555,     0.92646,     0.92642,     0.92639,     0.92635,     0.92632,     0.92628,\n            0.92625,     0.92621,     0.92618,     0.92614,     0.92611,     0.92607,     0.92604,       0.926,     0.92597,     0.92593,     0.92631,     0.92681,     0.92731,     0.92781,     0.92832,     0.92882,     0.92932,     0.92982,     0.93033,     0.93083,     0.93133,     0.93183,     0.93234,\n            0.93284,     0.93306,     0.93327,     0.93349,     0.93371,     0.93393,     0.93414,     0.93436,     0.93458,      0.9348,     0.93501,     0.93523,     0.93545,     0.93567,     0.93589,      0.9361,     0.93632,     0.93654,     0.93676,     0.93697,     0.93719,     0.93741,     0.93763,\n            0.93784,     0.93806,     0.93828,      0.9385,     0.93872,     0.93893,     0.93915,     0.93937,     0.93959,      0.9398,     0.93983,     0.93981,     0.93979,     0.93977,     0.93975,     0.93973,     0.93971,     0.93968,     0.93966,     0.93964,     0.93962,      0.9396,     0.93958,\n            0.93956,     0.93954,     0.93952,     0.93949,     0.93947,     0.93945,     0.93943,     0.93941,     0.93973,     0.94104,     0.94235,     0.94367,     0.94498,     0.94629,     0.94654,     0.94652,     0.94649,     0.94646,     0.94644,     0.94641,     0.94638,     0.94635,     0.94633,\n             0.9463,     0.94627,     0.94625,     0.94622,     0.94619,     0.94616,     0.94647,       0.947,     0.94752,     0.94805,     0.94858,      0.9491,     0.94963,     0.95015,     0.95068,     0.95121,     0.95173,     0.95226,     0.95278,     0.95331,     0.95348,     0.95346,     0.95345,\n            0.95343,     0.95342,     0.95341,     0.95339,     0.95338,     0.95336,     0.95335,     0.95333,     0.95332,      0.9533,     0.95329,     0.95327,     0.95326,     0.95325,     0.95323,     0.95322,      0.9532,     0.95319,     0.95317,     0.95316,     0.95314,     0.95313,     0.95454,\n            0.95645,     0.95836,     0.96026,     0.96154,     0.96266,     0.96379,     0.96491,     0.96603,     0.96716,     0.96825,     0.96823,      0.9682,     0.96817,     0.96814,     0.96811,     0.96808,     0.96806,     0.96803,     0.96801,     0.96849,     0.96898,     0.96946,     0.96995,\n            0.97044,     0.97092,     0.97141,     0.97189,     0.97238,     0.97286,     0.97335,     0.97384,     0.97432,     0.97481,     0.97529,     0.97578,     0.97579,     0.97577,     0.97575,     0.97573,     0.97572,      0.9757,     0.97568,     0.97566,     0.97564,     0.97563,     0.97561,\n            0.97559,     0.97557,     0.97555,     0.97554,     0.97552,      0.9755,     0.97548,     0.97546,     0.97544,     0.97543,     0.97541,     0.97539,     0.97537,     0.97536,     0.97534,     0.97532,      0.9753,     0.97528,     0.97527,     0.97525,     0.97523,     0.97521,      0.9752,\n            0.97518,     0.97516,     0.97515,     0.97513,     0.97512,      0.9751,     0.97508,     0.97507,     0.97505,     0.97504,     0.97502,       0.975,      0.9759,     0.97708,     0.97825,     0.97942,      0.9806,     0.98177,     0.98295,     0.98318,     0.98316,     0.98314,     0.98313,\n            0.98311,     0.98309,     0.98307,     0.98306,     0.98305,     0.98304,     0.98303,     0.98303,     0.98302,     0.98301,     0.98301,       0.983,     0.98299,     0.98299,     0.98298,     0.98297,     0.98297,     0.98296,     0.98295,     0.98295,     0.98294,     0.98294,     0.98293,\n            0.98292,     0.98292,     0.98291,     0.98288,     0.98284,      0.9828,     0.98276,     0.98273,     0.98271,     0.98269,     0.98267,     0.98265,     0.98263,     0.98246,     0.98244,     0.98243,     0.98242,     0.98241,      0.9824,     0.98238,     0.98237,     0.98236,     0.98235,\n            0.98233,     0.98232,     0.98231,     0.98229,     0.98224,      0.9822,     0.98215,     0.98213,     0.98211,      0.9821,     0.98208,     0.98206,     0.98205,     0.98203,     0.98201,       0.982,     0.98198,     0.98197,     0.98196,     0.98196,     0.98195,     0.98194,     0.98193,\n            0.98192,     0.98191,     0.98191,      0.9819,     0.98189,     0.98188,     0.98187,     0.98186,     0.98186,     0.98185,     0.98184,     0.98183,     0.98182,     0.98179,     0.98171,     0.98164,     0.98155,      0.9813,     0.98124,     0.98117,     0.98112,     0.98109,     0.98106,\n            0.98103,       0.981,     0.98097,     0.98093,     0.98087,     0.98081,     0.98076,     0.98071,     0.98066,     0.98062,     0.98058,     0.98055,     0.98051,     0.98048,     0.98045,     0.98042,     0.98039,     0.98027,     0.98013,       0.981,     0.98186,     0.98273,     0.98359,\n            0.98445,     0.98532,     0.98618,     0.98705,     0.98791,     0.98878,     0.98964,     0.98989,     0.98989,     0.98988,     0.98988,     0.98987,     0.98986,     0.98986,     0.98985,     0.98985,     0.98984,     0.98983,     0.98983,     0.98982,     0.98982,     0.98981,      0.9898,\n             0.9898,     0.98978,     0.98975,     0.98973,      0.9897,     0.98968,     0.98966,     0.98964,     0.98962,      0.9896,     0.98957,     0.98954,      0.9895,     0.98947,     0.98942,     0.98938,     0.98927,     0.98921,     0.98917,     0.98913,     0.98911,     0.98909,     0.98907,\n            0.98905,     0.98903,     0.98901,       0.989,     0.98899,     0.98898,     0.98897,     0.98896,     0.98896,     0.98895,     0.98894,     0.98893,     0.98892,     0.98891,      0.9889,     0.98889,     0.98888,     0.98886,     0.98885,     0.98883,     0.98882,      0.9888,     0.98879,\n            0.98877,     0.98875,     0.98872,     0.98868,     0.98865,     0.98847,     0.98839,     0.98815,     0.98802,      0.9878,     0.98774,     0.98768,     0.98742,     0.98717,     0.98711,     0.98705,     0.98699,     0.98694,     0.98689,     0.98684,      0.9868,     0.98676,     0.98672,\n            0.98667,      0.9865,     0.98625,     0.98616,     0.98571,     0.98548,     0.98525,     0.98501,      0.9849,     0.98461,     0.98451,      0.9844,     0.98435,     0.98431,     0.98428,     0.98424,      0.9842,     0.98417,     0.98413,     0.98406,     0.98399,     0.98392,     0.98383,\n            0.98371,     0.98357,      0.9834,     0.98324,     0.98308,     0.98293,     0.98278,     0.98272,     0.98267,     0.98263,     0.98258,     0.98253,     0.98249,     0.98148,     0.98134,     0.98119,     0.98073,     0.98054,     0.98035,     0.98015,      0.9799,     0.97958,     0.97928,\n            0.97896,     0.97849,      0.9781,     0.97788,     0.97764,     0.97737,     0.97714,     0.97694,     0.97674,     0.97636,      0.9758,      0.9755,     0.97534,     0.97518,     0.97503,     0.97421,     0.97387,     0.97223,     0.97192,     0.97163,     0.97046,     0.97009,     0.96971,\n            0.96926,     0.96881,       0.967,      0.9884,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,\n            0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96183,     0.96164,     0.96115,     0.96066,     0.96017,     0.95968,      0.9592,\n            0.95871,     0.95822,     0.95773,     0.95724,     0.95676,     0.95627,     0.95578,     0.95529,      0.9548,     0.95432,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,\n             0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,\n             0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,     0.95392,     0.95356,     0.95321,     0.95285,      0.9525,     0.95215,     0.95179,     0.95144,     0.95108,     0.95073,     0.95037,     0.95002,     0.94966,\n            0.94931,     0.94895,      0.9486,     0.94825,     0.94789,     0.94754,     0.94718,     0.94683,     0.94656,     0.94656,     0.94656,     0.94656,     0.94656,     0.94656,     0.94617,     0.94567,     0.94516,     0.94466,     0.94416,     0.94365,     0.94315,     0.94265,     0.94215,\n            0.94164,     0.94114,     0.94064,     0.94013,     0.93963,     0.93913,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93893,     0.93873,     0.93842,     0.93812,\n            0.93781,      0.9375,      0.9372,     0.93689,     0.93658,     0.93628,     0.93597,     0.93567,     0.93536,     0.93505,     0.93475,     0.93444,     0.93413,     0.93383,     0.93352,     0.93321,     0.93291,      0.9326,      0.9323,     0.93199,     0.93168,     0.93138,      0.9313,\n             0.9313,      0.9313,      0.9313,      0.9313,      0.9313,      0.9313,      0.9313,      0.9313,      0.9313,     0.93128,     0.93043,     0.92958,     0.92873,     0.92789,     0.92704,     0.92619,     0.92535,      0.9245,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,\n            0.92366,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,     0.92366,       0.923,     0.92229,     0.92158,     0.92088,     0.92017,     0.91947,     0.91876,     0.91805,     0.91735,     0.91664,     0.91594,\n            0.91525,     0.91456,     0.91387,     0.91318,     0.91249,      0.9118,     0.91111,     0.91042,     0.90973,     0.90904,     0.90836,     0.90769,     0.90702,     0.90635,     0.90568,     0.90501,     0.90435,     0.90368,     0.90301,     0.90234,     0.90167,       0.901,     0.90038,\n            0.89979,      0.8992,      0.8986,     0.89801,     0.89742,     0.89683,     0.89623,     0.89564,     0.89505,     0.89445,     0.89386,     0.89327,     0.89313,     0.89313,     0.89313,     0.89313,     0.89313,     0.89313,     0.89313,     0.89238,     0.89143,     0.89048,     0.88953,\n            0.88858,     0.88763,     0.88668,     0.88573,     0.88524,     0.88489,     0.88455,     0.88421,     0.88386,     0.88352,     0.88317,     0.88283,     0.88249,     0.88214,      0.8818,     0.88146,     0.88111,     0.88077,     0.88043,     0.88008,     0.87974,      0.8794,     0.87905,\n            0.87871,     0.87837,     0.87802,     0.87666,     0.87439,     0.87213,     0.87006,     0.86896,     0.86786,     0.86677,     0.86567,     0.86458,     0.86348,     0.85516,     0.85436,     0.85376,     0.85316,     0.85256,     0.85196,     0.85136,     0.85076,     0.85015,     0.84955,\n            0.84895,     0.84835,     0.84775,     0.84667,     0.84446,     0.84225,     0.84004,     0.83903,     0.83824,     0.83744,     0.83665,     0.83586,     0.83507,     0.83427,     0.83348,     0.83269,     0.83198,     0.83159,     0.83121,     0.83082,     0.83043,     0.83005,     0.82966,\n            0.82927,     0.82889,      0.8285,     0.82811,     0.82773,     0.82734,     0.82695,     0.82657,     0.82618,     0.82579,     0.82541,     0.82502,     0.82463,     0.82291,     0.81966,     0.81633,     0.81244,      0.8011,     0.79837,     0.79564,     0.79343,     0.79215,     0.79088,\n             0.7896,     0.78832,     0.78704,      0.7853,     0.78283,     0.78037,     0.77808,     0.77622,     0.77435,     0.77249,     0.77075,     0.76951,     0.76827,     0.76703,     0.76579,     0.76455,     0.76317,     0.75851,     0.74809,     0.74809,     0.74809,     0.74809,     0.74809,\n            0.74809,     0.74809,     0.74809,     0.74809,     0.74809,     0.74809,     0.74809,     0.74778,     0.74733,     0.74688,     0.74644,     0.74599,     0.74554,     0.74509,     0.74465,      0.7442,     0.74375,      0.7433,     0.74286,     0.74241,     0.74196,     0.74151,     0.74107,\n            0.74062,      0.7393,     0.73748,     0.73567,     0.73385,     0.73217,     0.73065,     0.72914,     0.72763,     0.72611,     0.72428,     0.72194,      0.7196,     0.71719,     0.71423,     0.71127,     0.70374,     0.69993,     0.69723,      0.6946,     0.69333,     0.69207,     0.69081,\n            0.68954,     0.68828,     0.68702,     0.68644,     0.68586,     0.68529,     0.68471,     0.68413,     0.68355,     0.68298,      0.6824,     0.68182,     0.68124,     0.68066,     0.68009,     0.67951,     0.67866,     0.67774,     0.67682,      0.6759,     0.67498,     0.67406,     0.67314,\n            0.67221,     0.67079,     0.66886,     0.66693,       0.665,     0.65436,     0.64986,     0.63632,     0.62979,     0.61825,      0.6151,     0.61195,     0.59935,     0.58721,     0.58444,     0.58167,     0.57915,     0.57691,     0.57468,     0.57246,     0.57064,     0.56882,       0.567,\n            0.56518,     0.55785,     0.54742,     0.54386,     0.52668,     0.51825,     0.50985,     0.50167,     0.49799,     0.48835,      0.4851,     0.48186,     0.48012,       0.479,     0.47787,     0.47675,     0.47563,      0.4745,     0.47338,     0.47134,     0.46921,     0.46708,     0.46447,\n            0.46088,     0.45705,     0.45221,     0.44773,     0.44347,     0.43954,     0.43569,     0.43412,     0.43294,     0.43177,      0.4306,     0.42943,     0.42826,     0.40452,     0.40142,     0.39832,     0.38859,     0.38475,     0.38087,     0.37687,     0.37224,     0.36614,     0.36081,\n            0.35523,     0.34727,     0.34101,     0.33754,     0.33377,     0.32972,      0.3264,      0.3235,     0.32059,     0.31535,     0.30789,     0.30393,     0.30197,     0.30002,     0.29806,     0.28844,     0.28453,     0.26725,     0.26431,     0.26144,     0.25082,      0.2476,     0.24438,\n            0.24077,     0.23714,     0.22372,     0.22137,     0.22045,     0.21904,     0.21764,     0.21623,     0.21483,     0.20564,     0.20184,     0.19798,     0.19355,     0.18891,     0.18394,      0.1773,     0.16149,     0.15057,     0.14248,     0.13706,     0.13595,     0.13484,     0.13374,\n            0.13263,     0.13152,     0.13041,     0.12717,     0.12107,     0.11515,     0.10653,     0.10395,     0.10137,    0.098442,    0.093902,    0.090796,    0.089162,    0.087529,    0.085895,    0.084261,    0.083457,    0.082834,     0.08221,    0.081587,    0.080963,     0.08034,    0.079716,\n           0.079093,    0.078469,    0.077846,    0.077222,    0.076599,    0.070265,    0.060041,    0.058437,    0.056832,    0.055228,    0.053624,    0.053052,    0.052619,    0.052185,    0.051751,    0.051317,    0.050884,     0.05045,    0.050016,    0.049583,    0.049149,    0.048715,    0.048281,\n           0.047848,    0.047414,     0.04698,    0.046547,    0.046113,    0.045688,    0.045284,     0.04488,    0.044477,    0.044073,     0.04367,    0.043266,    0.042862,    0.042459,    0.042055,    0.041652,    0.041248,    0.040844,    0.040441,    0.040037,    0.039633,     0.03923,    0.038826,\n           0.038423,    0.037696,    0.036418,     0.03514,    0.033862,    0.032584,    0.031306,    0.022117,    0.017641,    0.015103,    0.014755,    0.014406,    0.014057,    0.013708,    0.013359,     0.01301,    0.012661,    0.012313,    0.011964,    0.011615,    0.011266,    0.010917,    0.010568,\n           0.010219,   0.0098706,   0.0095217,   0.0091728,    0.008824,   0.0084751,   0.0081263,   0.0077774,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.7070292607113936\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.67798])\nnames: {0: 'Defect'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9532016422108175, 'metrics/recall(B)': 0.9329077814058758, 'metrics/mAP50(B)': 0.9684600505529565, 'metrics/mAP50-95(B)': 0.6779813951734421, 'fitness': 0.7070292607113936}\nsave_dir: PosixPath('runs/detect/val9')\nspeed: {'preprocess': 6.211792326083647, 'inference': 6.941962684780007, 'loss': 0.000964836960085212, 'postprocess': 1.422528076086169}\ntask: 'detect'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " # update with your model path\n",
        "\n",
        "# 3. Evaluation function\n",
        "def evaluate(name, yaml_path):\n",
        "    print(f\"\\n Evaluating on {name} set...\")\n",
        "    metrics = model.val(data=yaml_path, split=\"test\", save_json=False, save_txt=False, verbose=False)\n",
        "\n",
        "    p = metrics.box.p.mean().item()\n",
        "    r = metrics.box.r.mean().item()\n",
        "    map50 = metrics.box.map50.item()\n",
        "    map95 = metrics.box.map.item()\n",
        "    f1 = (2 * p * r) / (p + r + 1e-6)\n",
        "\n",
        "    return {\n",
        "        \"precision\": round(p, 4),\n",
        "        \"recall\": round(r, 4),\n",
        "        \"f1-score\": round(f1, 4),\n",
        "        \"accuracy (mAP@50)\": round(map50, 4),\n",
        "        \"mAP@50-95\": round(map95, 4)\n",
        "    }\n",
        "\n",
        "# 4. Run evaluation\n",
        "metrics_dict = {\n",
        "    \"Defect\": evaluate(\"Defect\", \"/kaggle/working/data_defect.yaml\"),\n",
        "    \"No Defect\": evaluate(\"No Defect\", \"/kaggle/working/data_no_defect.yaml\")\n",
        "}\n",
        "\n",
        "# 5. Print and save results\n",
        "print(\"\\nFinal Evaluation Metrics:\")\n",
        "print(json.dumps(metrics_dict, indent=4))\n",
        "\n",
        "with open(\"/kaggle/working/final_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics_dict, f, indent=4)"
      ],
      "metadata": {
        "id": "fvegGLlVSi2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import json\n",
        "metrics = model.val(data=\"/kaggle/working/data.yaml\", split=\"test\")\n",
        "\n",
        "# Extract correct metrics\n",
        "precision = metrics.box.p.mean().item()  # Mean Precision\n",
        "recall = metrics.box.r.mean().item()  # Mean Recall\n",
        "map50 = metrics.box.map50.item()  # mAP@50 (used as accuracy proxy)\n",
        "map = metrics.box.map.item()  # mAP@50-95\n",
        "\n",
        "# Calculate F1-score\n",
        "f1_score = (2 * precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "# Convert metrics to dictionary\n",
        "metrics_dict = {\n",
        "    \"Defect\": {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1-score\": f1_score,\n",
        "        \"accuracy (mAP@50)\": map50,\n",
        "        \"mAP@50-95\": map\n",
        "    }\n",
        "}\n",
        "# Pretty print the results\n",
        "print(json.dumps(metrics_dict, indent=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T09:48:02.917728Z",
          "iopub.execute_input": "2025-04-14T09:48:02.918024Z",
          "iopub.status.idle": "2025-04-14T09:48:19.684312Z",
          "shell.execute_reply.started": "2025-04-14T09:48:02.917999Z",
          "shell.execute_reply": "2025-04-14T09:48:19.683492Z"
        },
        "id": "cH3CrfQySXtn",
        "outputId": "2245bc4a-5c54-4d31-8fe8-6629ce743480"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.107 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/test-def/labels... 180 images, 88 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:01<00:00, 157.32it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/test-def is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:10<00:00,  1.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        131      0.939      0.933      0.959      0.641\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 2.7ms preprocess, 6.8ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1mruns/detect/val2\u001b[0m\n{\n    \"Defect\": {\n        \"precision\": 0.9385606197835207,\n        \"recall\": 0.9329077814058758,\n        \"f1-score\": 0.9357251632975606,\n        \"accuracy (mAP@50)\": 0.9586374027792293,\n        \"mAP@50-95\": 0.6405176562710763\n    }\n}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLO5m"
      ],
      "metadata": {
        "id": "8jReoWDWSXto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Check CUDA availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_yaml_path = \"/kaggle/working/data.yaml\"\n",
        "# Load YOLOv8 model with pre-trained weights\n",
        "model = YOLO(\"yolov5m.pt\")  # Using YOLOv8 weights\n",
        "\n",
        "# Train the model\n",
        "model.train(\n",
        "    data=data_yaml_path,\n",
        "    epochs=30,               # More training\n",
        "    batch=16,                # Larger batch\n",
        "    imgsz=960,               # Higher resolution\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    lr0=0.001, lrf=0.01,      # Learning rate decay\n",
        "    optimizer='SGD',         # Try SGD for detection tasks\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005,\n",
        "    cos_lr=True,             # Cyclic LR\n",
        "    augment=True,\n",
        "    freeze=[0, 1, 2],\n",
        "    amp=True                 # Enable mixed precision\n",
        ")\n",
        "\"\"\"\n",
        "\"YOLOv5m\": \"yolov5m.pt\",\n",
        "    \"YOLOv5l\": \"yolov5l.pt\",\n",
        "    \"YOLOv6-L\": \"yolov6l.pt\",\n",
        "    \"YOLOv7-X\": \"yolov7x.pt\",\n",
        "    \"YOLOv8-L\": \"yolov8l.pt\",\n",
        "    \"YOLO-NAS\": \"yolo_nas_l\"\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T18:52:27.076364Z",
          "iopub.execute_input": "2025-03-25T18:52:27.076707Z",
          "iopub.status.idle": "2025-03-25T19:55:55.917416Z",
          "shell.execute_reply.started": "2025-03-25T18:52:27.076682Z",
          "shell.execute_reply": "2025-03-25T19:55:55.916312Z"
        },
        "id": "9Gp4ElXLSXto",
        "outputId": "11792c96-c4db-44be-defb-0c1f0ce69802"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PRO TIP ğŸ’¡ Replace 'model=yolov5m.pt' with new 'model=yolov5mu.pt'.\nYOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n\nUltralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5m.pt, data=/kaggle/working/data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=[0, 1, 2], multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      5280  ultralytics.nn.modules.conv.Conv             [3, 48, 6, 2, 2]              \n  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n  2                  -1  2     65280  ultralytics.nn.modules.block.C3              [96, 96, 2]                   \n  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n  4                  -1  4    444672  ultralytics.nn.modules.block.C3              [192, 192, 4]                 \n  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n  6                  -1  6   2512896  ultralytics.nn.modules.block.C3              [384, 384, 6]                 \n  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n  8                  -1  2   4134912  ultralytics.nn.modules.block.C3              [768, 768, 2]                 \n  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n 10                  -1  1    295680  ultralytics.nn.modules.conv.Conv             [768, 384, 1, 1]              \n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 13                  -1  2   1182720  ultralytics.nn.modules.block.C3              [768, 384, 2, False]          \n 14                  -1  1     74112  ultralytics.nn.modules.conv.Conv             [384, 192, 1, 1]              \n 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 17                  -1  2    296448  ultralytics.nn.modules.block.C3              [384, 192, 2, False]          \n 18                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 20                  -1  2   1035264  ultralytics.nn.modules.block.C3              [384, 384, 2, False]          \n 21                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 23                  -1  2   4134912  ultralytics.nn.modules.block.C3              [768, 768, 2, False]          \n 24        [17, 20, 23]  1   4218643  ultralytics.nn.modules.head.Detect           [1, [192, 384, 768]]          \nYOLOv5m summary: 197 layers, 25,065,715 parameters, 25,065,699 gradients, 64.4 GFLOPs\n\nTransferred 553/559 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\nFreezing layer 'model.0.conv.weight'\nFreezing layer 'model.0.bn.weight'\nFreezing layer 'model.0.bn.bias'\nFreezing layer 'model.1.conv.weight'\nFreezing layer 'model.1.bn.weight'\nFreezing layer 'model.1.bn.bias'\nFreezing layer 'model.2.cv1.conv.weight'\nFreezing layer 'model.2.cv1.bn.weight'\nFreezing layer 'model.2.cv1.bn.bias'\nFreezing layer 'model.2.cv2.conv.weight'\nFreezing layer 'model.2.cv2.bn.weight'\nFreezing layer 'model.2.cv2.bn.bias'\nFreezing layer 'model.2.cv3.conv.weight'\nFreezing layer 'model.2.cv3.bn.weight'\nFreezing layer 'model.2.cv3.bn.bias'\nFreezing layer 'model.2.m.0.cv1.conv.weight'\nFreezing layer 'model.2.m.0.cv1.bn.weight'\nFreezing layer 'model.2.m.0.cv1.bn.bias'\nFreezing layer 'model.2.m.0.cv2.conv.weight'\nFreezing layer 'model.2.m.0.cv2.bn.weight'\nFreezing layer 'model.2.m.0.cv2.bn.bias'\nFreezing layer 'model.2.m.1.cv1.conv.weight'\nFreezing layer 'model.2.m.1.cv1.bn.weight'\nFreezing layer 'model.2.m.1.cv1.bn.bias'\nFreezing layer 'model.2.m.1.cv2.conv.weight'\nFreezing layer 'model.2.m.1.cv2.bn.weight'\nFreezing layer 'model.2.m.1.cv2.bn.bias'\nFreezing layer 'model.24.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/labels... 1440 images, 720 backgrounds, 4 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1440/1440 [00:01<00:00, 796.60it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_28_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_437_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_732_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_896_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/train-def is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 698.31it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Plotting labels to runs/detect/train2/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.001, momentum=0.937) with parameter groups 91 weight(decay=0.0), 98 weight(decay=0.0005), 97 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 960 train, 960 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/train2\u001b[0m\nStarting training for 30 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       1/30      10.2G      3.307      6.168      3.238          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:04<00:00,  1.38s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.05s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116    0.00157      0.707    0.00465   0.000918\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       2/30        12G      2.599      3.768      2.858          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:03<00:00,  1.37s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.05s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.231      0.259       0.13     0.0387\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       3/30        12G      2.145      2.733       2.37          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.02s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.547      0.388       0.42      0.167\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       4/30        12G      1.969      2.205      2.124         18        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:57<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.05s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.694      0.469      0.593      0.261\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       5/30        12G      1.794      1.853      1.947         16        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.31s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.02s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.772      0.569      0.704      0.332\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       6/30        12G      1.711      1.585      1.871         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:57<00:00,  1.31s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.00s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.635      0.733      0.741      0.378\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       7/30        12G      1.618      1.383      1.778         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:56<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.753      0.684      0.768      0.383\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       8/30        12G      1.569      1.283      1.669         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.14s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.825       0.73      0.841      0.428\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       9/30        12G      1.498       1.15      1.594          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:56<00:00,  1.29s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.01s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.865      0.802      0.877      0.472\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      10/30        12G      1.425      1.053      1.519          4        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.31s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.00it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.891       0.81      0.912      0.538\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      11/30        12G      1.386     0.9721      1.464         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.00s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.874      0.871      0.931      0.513\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      12/30        12G      1.321     0.8911      1.405          4        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.04s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.936      0.877      0.937      0.547\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      13/30        12G      1.306     0.8894      1.405         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:56<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.07s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.953       0.88      0.959      0.569\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      14/30        12G      1.242     0.8357      1.335         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.00s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.964       0.92      0.966      0.575\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      15/30        12G      1.193     0.7606      1.305         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.04s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.937      0.871      0.958      0.561\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      16/30        12G      1.193     0.7657      1.302         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.18s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.963      0.909       0.97      0.561\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      17/30        12G      1.142     0.7072      1.264         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:57<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.05s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.944      0.931      0.971      0.582\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      18/30        12G      1.126     0.7084      1.223         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:56<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.01s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.922      0.931      0.971      0.557\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      19/30        12G      1.105     0.6699       1.21          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.31s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.964       0.94      0.984      0.624\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      20/30        12G      1.045     0.6402      1.179         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:57<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.98      0.931      0.982      0.646\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      21/30        12G      1.006     0.5538       1.18          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:04<00:00,  1.39s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.11s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.973       0.92       0.98      0.609\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      22/30        12G     0.9662     0.5087      1.129          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:56<00:00,  1.29s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.01it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.955      0.912      0.982      0.603\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      23/30        12G     0.9386     0.5097      1.126          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:55<00:00,  1.28s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.08s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.963      0.931      0.977      0.651\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      24/30        12G     0.9076     0.4831      1.103          5        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:56<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.05s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.958       0.94      0.982      0.639\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      25/30        12G     0.8644     0.4786      1.078          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:56<00:00,  1.30s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.07s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.965      0.966      0.985      0.648\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      26/30        12G     0.8592     0.4664      1.084          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:59<00:00,  1.33s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.09s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.972      0.966      0.982      0.651\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      27/30        12G     0.8486     0.4612      1.062          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.32s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.01s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.965       0.96      0.983      0.644\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      28/30        12G      0.868     0.4567      1.088          3        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:59<00:00,  1.33s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.05s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.973      0.966      0.984       0.66\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      29/30        12G     0.8516     0.4508      1.068          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:00<00:00,  1.34s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.08s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.974      0.961      0.983      0.653\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      30/30        12G     0.8248     0.4392      1.061          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:57<00:00,  1.31s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.01it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.97      0.966      0.985      0.658\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n30 epochs completed in 1.049 hours.\nOptimizer stripped from runs/detect/train2/weights/last.pt, 50.5MB\nOptimizer stripped from runs/detect/train2/weights/best.pt, 50.5MB\n\nValidating runs/detect/train2/weights/best.pt...\nUltralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\nYOLOv5m summary (fused): 106 layers, 25,045,795 parameters, 0 gradients, 64.0 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:10<00:00,  1.77s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.94      0.951      0.979      0.703\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 0.3ms preprocess, 40.7ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1mruns/detect/train2\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n\"YOLOv5m\": \"yolov5m.pt\",\\n    \"YOLOv5l\": \"yolov5l.pt\",\\n    \"YOLOv6-L\": \"yolov6l.pt\",\\n    \"YOLOv7-X\": \"yolov7x.pt\",\\n    \"YOLOv8-L\": \"yolov8l.pt\",\\n    \"YOLO-NAS\": \"yolo_nas_l\"\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "xn4bkLI9SXtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model.val(data=\"/kaggle/working/data.yaml\", split=\"test\")\n",
        "\n",
        "# Extract correct metrics\n",
        "precision = metrics.box.p.mean().item()  # Mean Precision\n",
        "recall = metrics.box.r.mean().item()  # Mean Recall\n",
        "map50 = metrics.box.map50.item()  # mAP@50 (used as accuracy proxy)\n",
        "map = metrics.box.map.item()  # mAP@50-95\n",
        "\n",
        "# Calculate F1-score\n",
        "f1_score = (2 * precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "# Convert metrics to dictionary\n",
        "metrics_dict = {\n",
        "    \"Defect\": {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1-score\": f1_score,\n",
        "        \"accuracy (mAP@50)\": map50,\n",
        "        \"mAP@50-95\": map\n",
        "    }\n",
        "}\n",
        "# Pretty print the results\n",
        "print(json.dumps(metrics_dict, indent=4))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T19:56:10.040225Z",
          "iopub.execute_input": "2025-03-25T19:56:10.040525Z",
          "iopub.status.idle": "2025-03-25T19:56:23.408263Z",
          "shell.execute_reply.started": "2025-03-25T19:56:10.040500Z",
          "shell.execute_reply": "2025-03-25T19:56:23.407052Z"
        },
        "id": "B9sWQ2FUSXtp",
        "outputId": "2ac803ba-a0c4-4c7a-8bf1-5889dc40317c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/test-def/labels... 180 images, 88 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 567.62it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/test-def is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:09<00:00,  1.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        131      0.969      0.969      0.981      0.706\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 0.3ms preprocess, 14.6ms inference, 0.0ms loss, 0.6ms postprocess per image\nResults saved to \u001b[1mruns/detect/val9\u001b[0m\n{\n    \"Defect\": {\n        \"precision\": 0.9690145619066801,\n        \"recall\": 0.9694656488549618,\n        \"f1-score\": 0.9692395528968367,\n        \"accuracy (mAP@50)\": 0.9805863562310793,\n        \"mAP@50-95\": 0.7058635572632079\n    }\n}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model"
      ],
      "metadata": {
        "id": "teasBRrnSXtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"/kaggle/working/runs_back5mmyes\", 'zip', \"/kaggle/working/runs\")\n",
        "print(\"Runs folder saved as runs_backup.zip in Kaggle working directory.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:21:40.671419Z",
          "iopub.execute_input": "2025-03-25T20:21:40.671769Z",
          "iopub.status.idle": "2025-03-25T20:21:47.239152Z",
          "shell.execute_reply.started": "2025-03-25T20:21:40.671746Z",
          "shell.execute_reply": "2025-03-25T20:21:47.238236Z"
        },
        "id": "vjfEI7fQSXtq",
        "outputId": "c5807d6d-1be8-435e-e467-5221bd947c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Runs folder saved as runs_backup.zip in Kaggle working directory.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolov5l"
      ],
      "metadata": {
        "id": "W5TqLdOMSXtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov5l.pt\")  # Using YOLOv8 weights\n",
        "\n",
        "model.train(\n",
        "    data=data_yaml_path,\n",
        "    epochs=30,               # More training\n",
        "    batch=16,                # Larger batch\n",
        "    imgsz=960,               # Higher resolution\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    lr0=0.001, lrf=0.01,      # Learning rate decay\n",
        "    optimizer='SGD',         # Try SGD for detection tasks\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005,\n",
        "    cos_lr=True,             # Cyclic LR\n",
        "    augment=True,\n",
        "    freeze=[0, 1, 2],\n",
        "    amp=True                 # Enable mixed precision\n",
        ")\n",
        "\"\"\"\n",
        "\"YOLOv5m\": \"yolov5m.pt\",\n",
        "    \"YOLOv5l\": \"yolov5l.pt\",\n",
        "    \"YOLOv6-L\": \"yolov6l.pt\",\n",
        "    \"YOLOv7-X\": \"yolov7x.pt\",\n",
        "    \"YOLOv8-L\": \"yolov8l.pt\",\n",
        "    \"YOLO-NAS\": \"yolo_nas_l\"\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T09:02:25.279967Z",
          "iopub.execute_input": "2025-03-26T09:02:25.280435Z",
          "iopub.status.idle": "2025-03-26T10:13:24.559018Z",
          "shell.execute_reply.started": "2025-03-26T09:02:25.280413Z",
          "shell.execute_reply": "2025-03-26T10:13:24.558102Z"
        },
        "id": "KPe8_C4zSXtr",
        "outputId": "2af645a4-6cf3-4e8a-8f67-9cfe4acbe7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PRO TIP ğŸ’¡ Replace 'model=yolov5l.pt' with new 'model=yolov5lu.pt'.\nYOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5lu.pt to 'yolov5lu.pt'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102M/102M [00:01<00:00, 79.1MB/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5l.pt, data=/kaggle/working/data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=[0, 1, 2], multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 23.3MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Overriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      7040  ultralytics.nn.modules.conv.Conv             [3, 64, 6, 2, 2]              \n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  2                  -1  3    156928  ultralytics.nn.modules.block.C3              [128, 128, 3]                 \n  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  4                  -1  6   1118208  ultralytics.nn.modules.block.C3              [256, 256, 6]                 \n  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  6                  -1  9   6433792  ultralytics.nn.modules.block.C3              [512, 512, 9]                 \n  7                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 2]             \n  8                  -1  3   9971712  ultralytics.nn.modules.block.C3              [1024, 1024, 3]               \n  9                  -1  1   2624512  ultralytics.nn.modules.block.SPPF            [1024, 1024, 5]               \n 10                  -1  1    525312  ultralytics.nn.modules.conv.Conv             [1024, 512, 1, 1]             \n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 13                  -1  3   2757632  ultralytics.nn.modules.block.C3              [1024, 512, 3, False]         \n 14                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 17                  -1  3    690688  ultralytics.nn.modules.block.C3              [512, 256, 3, False]          \n 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 20                  -1  3   2495488  ultralytics.nn.modules.block.C3              [512, 512, 3, False]          \n 21                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 23                  -1  3   9971712  ultralytics.nn.modules.block.C3              [1024, 1024, 3, False]        \n 24        [17, 20, 23]  1   7058131  ultralytics.nn.modules.head.Detect           [1, [256, 512, 1024]]         \nYOLOv5l summary: 241 layers, 53,164,115 parameters, 53,164,099 gradients, 135.3 GFLOPs\n\nTransferred 685/691 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\nFreezing layer 'model.0.conv.weight'\nFreezing layer 'model.0.bn.weight'\nFreezing layer 'model.0.bn.bias'\nFreezing layer 'model.1.conv.weight'\nFreezing layer 'model.1.bn.weight'\nFreezing layer 'model.1.bn.bias'\nFreezing layer 'model.2.cv1.conv.weight'\nFreezing layer 'model.2.cv1.bn.weight'\nFreezing layer 'model.2.cv1.bn.bias'\nFreezing layer 'model.2.cv2.conv.weight'\nFreezing layer 'model.2.cv2.bn.weight'\nFreezing layer 'model.2.cv2.bn.bias'\nFreezing layer 'model.2.cv3.conv.weight'\nFreezing layer 'model.2.cv3.bn.weight'\nFreezing layer 'model.2.cv3.bn.bias'\nFreezing layer 'model.2.m.0.cv1.conv.weight'\nFreezing layer 'model.2.m.0.cv1.bn.weight'\nFreezing layer 'model.2.m.0.cv1.bn.bias'\nFreezing layer 'model.2.m.0.cv2.conv.weight'\nFreezing layer 'model.2.m.0.cv2.bn.weight'\nFreezing layer 'model.2.m.0.cv2.bn.bias'\nFreezing layer 'model.2.m.1.cv1.conv.weight'\nFreezing layer 'model.2.m.1.cv1.bn.weight'\nFreezing layer 'model.2.m.1.cv1.bn.bias'\nFreezing layer 'model.2.m.1.cv2.conv.weight'\nFreezing layer 'model.2.m.1.cv2.bn.weight'\nFreezing layer 'model.2.m.1.cv2.bn.bias'\nFreezing layer 'model.2.m.2.cv1.conv.weight'\nFreezing layer 'model.2.m.2.cv1.bn.weight'\nFreezing layer 'model.2.m.2.cv1.bn.bias'\nFreezing layer 'model.2.m.2.cv2.conv.weight'\nFreezing layer 'model.2.m.2.cv2.bn.weight'\nFreezing layer 'model.2.m.2.cv2.bn.bias'\nFreezing layer 'model.24.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 106MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/labels... 1440 images, 720 backgrounds, 4 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1440/1440 [00:13<00:00, 104.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_28_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_437_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_732_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_896_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/train-def is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:02<00:00, 87.50it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Plotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.001, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 960 train, 960 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 30 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       1/30      15.1G      3.135      6.102      3.274         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:13<00:00,  1.49s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.06s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116    0.00163       0.75    0.00498    0.00112\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       2/30      15.2G      2.627      3.725      2.892         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.47s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.365      0.233       0.19     0.0687\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       3/30      14.6G       2.19      2.736      2.427          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.411      0.422      0.339      0.121\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       4/30      14.7G      1.953       2.16      2.147         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.538      0.582      0.554      0.259\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       5/30      14.6G      1.777      1.841      2.013         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.653      0.681      0.714      0.357\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       6/30      14.7G      1.652       1.52      1.854         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.19it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.808      0.761      0.808       0.43\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       7/30      14.5G      1.574      1.313      1.733         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.781      0.767       0.84      0.422\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       8/30      14.7G      1.522      1.141      1.659         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.786      0.802      0.836       0.45\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       9/30      14.5G      1.436      1.051      1.558         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.938      0.782      0.903      0.547\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      10/30      14.6G      1.362     0.9338      1.474         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.913      0.862      0.922      0.557\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      11/30      14.6G      1.356     0.9046      1.457         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.913      0.828      0.929      0.564\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      12/30      14.7G      1.255     0.8481      1.378          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.21it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.939      0.888      0.951      0.572\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      13/30      14.6G      1.186     0.7474      1.307         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.927      0.897      0.944      0.609\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      14/30      14.7G      1.162     0.7272      1.308          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.21it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.901      0.942       0.95      0.573\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      15/30      14.5G      1.119     0.6911       1.26          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.14it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.887      0.943      0.969      0.602\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      16/30      14.7G      1.073     0.6436      1.209         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.947      0.914      0.968      0.592\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      17/30      14.5G      1.046     0.6305      1.191         19        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.901      0.936      0.965      0.615\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      18/30      14.7G      1.024     0.6277      1.169         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.935      0.922      0.977      0.637\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      19/30      14.6G     0.9561     0.5846      1.127         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.17it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.956      0.937      0.984      0.654\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      20/30      14.7G     0.9452     0.5492       1.13         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.23it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.966      0.905      0.984      0.628\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      21/30      14.5G     0.9209     0.4878      1.118          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:16<00:00,  1.52s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.17it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.947      0.957      0.984      0.659\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      22/30      14.7G     0.8877     0.4734      1.097          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.931      0.966      0.983      0.647\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      23/30      14.6G     0.8172     0.4374      1.061          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.17it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.933      0.954      0.986      0.666\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      24/30      14.7G     0.8088     0.4261      1.031          5        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.21it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.94      0.942      0.985      0.638\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      25/30      14.5G     0.7655     0.3929      1.021          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.14it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.956      0.937      0.986      0.674\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      26/30      14.7G     0.7624     0.4167      1.033          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.21it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.963      0.931      0.988      0.669\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      27/30      14.5G     0.7583     0.3843      1.005          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.18it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.97      0.931      0.986      0.658\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      28/30      14.7G     0.7676     0.3888      1.022          3        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.23it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.965      0.944      0.987      0.679\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      29/30      14.6G     0.7224     0.3872      1.003          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:11<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.17it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.953      0.948      0.987      0.673\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      30/30      14.6G     0.7051     0.3646     0.9967          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:10<00:00,  1.46s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.23it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.971      0.931      0.986      0.671\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n30 epochs completed in 1.162 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 106.8MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 106.8MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nYOLOv5l summary (fused): 128 layers, 53,132,179 parameters, 0 gradients, 134.7 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:11<00:00,  1.94s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.941       0.96      0.984      0.726\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 0.3ms preprocess, 58.0ms inference, 0.0ms loss, 1.5ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n\"YOLOv5m\": \"yolov5m.pt\",\\n    \"YOLOv5l\": \"yolov5l.pt\",\\n    \"YOLOv6-L\": \"yolov6l.pt\",\\n    \"YOLOv7-X\": \"yolov7x.pt\",\\n    \"YOLOv8-L\": \"yolov8l.pt\",\\n    \"YOLO-NAS\": \"yolo_nas_l\"\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "TizHsFDKSXts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your trained model (change this to the correct path)\n",
        "model_path = '/kaggle/working/yolov5/runs/detect/train/weights/best.pt'  # Replace with the correct path to the best model\n",
        "# Evaluate the model\n",
        "model = YOLO(model_path)\n",
        "# Evaluate on the validation dataset (it automatically uses the val split from your YAML file)\n",
        "results = model.val(data=data_yaml_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T10:13:24.560400Z",
          "iopub.execute_input": "2025-03-26T10:13:24.560678Z",
          "iopub.status.idle": "2025-03-26T10:13:39.330079Z",
          "shell.execute_reply.started": "2025-03-26T10:13:24.560629Z",
          "shell.execute_reply": "2025-03-26T10:13:39.328757Z"
        },
        "id": "RV71H98oSXtt",
        "outputId": "a316c7a9-b43b-406f-e5f8-0d797b46be10"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nYOLOv5l summary (fused): 128 layers, 53,132,179 parameters, 0 gradients, 134.7 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 685.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.965      0.943      0.987      0.682\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 3.9ms preprocess, 25.9ms inference, 0.0ms loss, 0.9ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "metrics = model.val(data=\"/kaggle/working/data.yaml\", split=\"test\")\n",
        "\n",
        "# Extract correct metrics\n",
        "precision = metrics.box.p.mean().item()  # Mean Precision\n",
        "recall = metrics.box.r.mean().item()  # Mean Recall\n",
        "map50 = metrics.box.map50.item()  # mAP@50 (used as accuracy proxy)\n",
        "map = metrics.box.map.item()  # mAP@50-95\n",
        "\n",
        "# Calculate F1-score\n",
        "f1_score = (2 * precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "# Convert metrics to dictionary\n",
        "metrics_dict = {\n",
        "    \"Defect\": {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1-score\": f1_score,\n",
        "        \"accuracy (mAP@50)\": map50,\n",
        "        \"mAP@50-95\": map\n",
        "    }\n",
        "}\n",
        "# Pretty print the results\n",
        "print(json.dumps(metrics_dict, indent=4))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T10:13:39.331724Z",
          "iopub.execute_input": "2025-03-26T10:13:39.332026Z",
          "iopub.status.idle": "2025-03-26T10:13:58.608558Z",
          "shell.execute_reply.started": "2025-03-26T10:13:39.332002Z",
          "shell.execute_reply": "2025-03-26T10:13:58.607761Z"
        },
        "id": "1ll9ZJGWSXtu",
        "outputId": "bc82a2f4-3487-49e2-f881-599cc30d8698"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/test-def/labels... 180 images, 88 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:01<00:00, 94.35it/s] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/test-def is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        131      0.953      0.969      0.989      0.724\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 2.6ms preprocess, 25.9ms inference, 0.0ms loss, 1.0ms postprocess per image\nResults saved to \u001b[1mruns/detect/val2\u001b[0m\n{\n    \"Defect\": {\n        \"precision\": 0.9529406573951867,\n        \"recall\": 0.9694656488549618,\n        \"f1-score\": 0.9611316288054991,\n        \"accuracy (mAP@50)\": 0.9887115220638859,\n        \"mAP@50-95\": 0.7236408002717768\n    }\n}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolo8s"
      ],
      "metadata": {
        "id": "w61D2Bs2SXtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Check CUDA availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_yaml_path = \"/kaggle/working/data.yaml\"\n",
        "model = YOLO(\"yolov8s.pt\")  # Using YOLOv8 weights\n",
        "\n",
        "model.train(\n",
        "     data=data_yaml_path,\n",
        "    epochs=30,               # More training\n",
        "    batch=16,                # Larger batch\n",
        "    imgsz=960,               # Higher resolution\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    lr0=0.001, lrf=0.01,      # Learning rate decay\n",
        "    optimizer='SGD',         # Try SGD for detection tasks\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005,\n",
        "    cos_lr=True,             # Cyclic LR\n",
        "    augment=True,\n",
        "    freeze=[0, 1, 2],\n",
        "    amp=True                 # Enable mixed precision\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T10:13:58.609884Z",
          "iopub.execute_input": "2025-03-26T10:13:58.610158Z",
          "iopub.status.idle": "2025-03-26T10:56:36.907954Z",
          "shell.execute_reply.started": "2025-03-26T10:13:58.610135Z",
          "shell.execute_reply": "2025-03-26T10:56:36.906735Z"
        },
        "id": "WnoH1my4SXtv",
        "outputId": "4e0b28a4-6533-447f-f459-2423941a182f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.5M/21.5M [00:00<00:00, 232MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/kaggle/working/data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=[0, 1, 2], multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \nModel summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n\nTransferred 349/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\nFreezing layer 'model.0.conv.weight'\nFreezing layer 'model.0.bn.weight'\nFreezing layer 'model.0.bn.bias'\nFreezing layer 'model.1.conv.weight'\nFreezing layer 'model.1.bn.weight'\nFreezing layer 'model.1.bn.bias'\nFreezing layer 'model.2.cv1.conv.weight'\nFreezing layer 'model.2.cv1.bn.weight'\nFreezing layer 'model.2.cv1.bn.bias'\nFreezing layer 'model.2.cv2.conv.weight'\nFreezing layer 'model.2.cv2.bn.weight'\nFreezing layer 'model.2.cv2.bn.bias'\nFreezing layer 'model.2.m.0.cv1.conv.weight'\nFreezing layer 'model.2.m.0.cv1.bn.weight'\nFreezing layer 'model.2.m.0.cv1.bn.bias'\nFreezing layer 'model.2.m.0.cv2.conv.weight'\nFreezing layer 'model.2.m.0.cv2.bn.weight'\nFreezing layer 'model.2.m.0.cv2.bn.bias'\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/labels... 1440 images, 720 backgrounds, 4 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1440/1440 [00:01<00:00, 748.77it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_28_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_437_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_732_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_896_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/train-def is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 443.99it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\nPlotting labels to runs/detect/train2/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.001, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 960 train, 960 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train2\u001b[0m\nStarting training for 30 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       1/30      9.92G      3.573      12.43      3.407         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:21<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.10it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116    0.00159        0.5    0.00306   0.000687\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       2/30      9.94G      2.875      3.743      2.961         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:27<00:00,  1.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.27it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.205      0.172     0.0981     0.0293\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       3/30      9.94G      2.354       2.83       2.52          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:21<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.63it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.342       0.31      0.241     0.0959\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       4/30      9.94G      2.086      2.406      2.205         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:20<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.30it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.37      0.491      0.351      0.156\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       5/30      9.94G      1.922        2.1      2.087         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:17<00:00,  1.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.60it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.65      0.543      0.589      0.257\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       6/30      9.94G      1.825       1.77       1.93         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:16<00:00,  1.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.48it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.694      0.603      0.675      0.356\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       7/30      9.94G      1.738       1.57      1.817         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:16<00:00,  1.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.28it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.813       0.71        0.8      0.441\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       8/30      9.94G      1.673      1.406       1.73         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:20<00:00,  1.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.36it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.738      0.778      0.802      0.421\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       9/30      9.94G      1.581       1.28      1.627         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:20<00:00,  1.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.79it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.759      0.758      0.821      0.477\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      10/30      9.94G      1.539      1.163      1.554         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:17<00:00,  1.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.25it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.854      0.802      0.878      0.485\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      11/30      9.94G      1.532      1.133      1.557         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:16<00:00,  1.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.47it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.826      0.793       0.85        0.5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      12/30      9.94G      1.443      1.067      1.464          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:18<00:00,  1.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.64it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.847      0.845      0.888      0.505\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      13/30      9.94G       1.41     0.9733      1.447         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:19<00:00,  1.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.80it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.912      0.808      0.905      0.549\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      14/30      9.94G      1.367      0.951      1.418          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:16<00:00,  1.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.42it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.884      0.879      0.912      0.522\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      15/30      9.94G      1.343     0.9181      1.397          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:18<00:00,  1.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.75it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.916      0.828      0.931      0.565\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      16/30      9.94G      1.266     0.8573       1.33         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:18<00:00,  1.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.55it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.839      0.899      0.932      0.561\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      17/30      9.94G      1.266     0.8088      1.323         19        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:17<00:00,  1.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.56it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.879      0.879       0.93      0.558\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      18/30      9.94G      1.242     0.8286      1.296         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:17<00:00,  1.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.24it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.939      0.862      0.939      0.608\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      19/30      9.94G      1.202     0.7801      1.273         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:16<00:00,  1.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.48it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.932      0.888      0.944      0.595\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      20/30      9.94G      1.185     0.7741      1.271         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:18<00:00,  1.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.50it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.912      0.896       0.95      0.604\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      21/30      9.94G      1.211      0.725      1.303          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:27<00:00,  1.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.41it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.937      0.862      0.942      0.571\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      22/30      9.94G      1.134     0.6797       1.27          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:26<00:00,  1.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.29it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.926      0.866      0.948      0.581\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      23/30      9.94G      1.125     0.6723      1.252          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:28<00:00,  1.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.35it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.944      0.867      0.953      0.598\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      24/30      9.94G      1.071     0.6384      1.211          5        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:21<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.08it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.94      0.871      0.952      0.608\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      25/30      9.94G      1.051     0.5947      1.205          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:18<00:00,  1.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.45it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.945      0.891      0.954      0.621\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      26/30      9.94G      1.073      0.605      1.224          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:21<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.69it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.954      0.871      0.954      0.615\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      27/30      9.94G      1.033      0.593      1.191          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:20<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.47it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.959      0.879      0.954      0.602\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      28/30      9.94G      1.061     0.6038      1.213          3        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:21<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.58it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.962      0.877      0.954      0.624\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      29/30      9.94G      1.016     0.5914      1.183          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:18<00:00,  1.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.43it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.953      0.876      0.955      0.639\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      30/30      9.94G      1.027     0.5758      1.193          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:18<00:00,  1.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.29it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.937       0.89      0.955      0.633\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n30 epochs completed in 0.700 hours.\nOptimizer stripped from runs/detect/train2/weights/last.pt, 22.5MB\nOptimizer stripped from runs/detect/train2/weights/best.pt, 22.5MB\n\nValidating runs/detect/train2/weights/best.pt...\nUltralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.44s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.946      0.898      0.956       0.67\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 0.4ms preprocess, 20.0ms inference, 0.0ms loss, 1.9ms postprocess per image\nResults saved to \u001b[1mruns/detect/train2\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "ultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([0])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x796f6cf1b400>\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,\n            0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,\n            0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,     0.97938,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,\n             0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,      0.9703,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,\n            0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.95327,\n            0.95327,     0.95327,     0.95327,     0.95327,     0.95327,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,\n            0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.94595,     0.89831,     0.89831,     0.89831,     0.89831,     0.89831,     0.89831,     0.89831,     0.89831,     0.87705,     0.87705,     0.87705,     0.87705,     0.87705,     0.87705,     0.87705,\n            0.87705,     0.87705,     0.85714,     0.85714,     0.85714,     0.85714,     0.85714,     0.85714,     0.85714,     0.85714,     0.85714,     0.74658,     0.74658,     0.74658,     0.74658,     0.74658,     0.74658,     0.74658,     0.74658,     0.73333,     0.73333,     0.73333,     0.73333,\n            0.73333,     0.73333,     0.73333,     0.73333,     0.73333,     0.56061,     0.56061,     0.56061,     0.56061,     0.56061,     0.56061,     0.56061,     0.56061,      0.5045,      0.5045,      0.5045,      0.5045,      0.5045,      0.5045,      0.5045,      0.5045,      0.5045,     0.44141,\n            0.44141,     0.44141,     0.44141,     0.44141,     0.44141,     0.44141,     0.44141,     0.44141,     0.39041,     0.39041,     0.39041,     0.39041,     0.39041,     0.39041,     0.39041,     0.39041,     0.29487,     0.29487,     0.29487,     0.29487,     0.29487,     0.29487,     0.29487,\n            0.29487,     0.29487,    0.098914,     0.08655,    0.074185,    0.061821,    0.049457,    0.037093,    0.024728,    0.012364,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.19231,     0.19231,     0.26836,     0.32576,     0.36426,     0.39807,     0.42913,     0.44224,     0.45706,     0.47931,     0.49244,     0.50683,      0.5153,     0.52999,     0.53789,     0.54784,     0.55018,     0.55649,     0.55825,     0.56954,     0.57507,     0.58567,     0.58883,\n            0.59691,     0.59983,     0.60438,      0.6039,     0.60595,     0.61213,     0.61559,     0.62229,     0.62366,     0.62757,     0.62798,     0.62839,     0.62879,      0.6292,     0.63211,     0.63413,     0.63782,     0.63922,     0.64084,     0.64681,     0.64886,      0.6519,     0.66204,\n             0.6652,     0.66694,     0.66747,       0.668,     0.66853,     0.66998,     0.67194,     0.67294,     0.67327,      0.6736,     0.67394,     0.67427,      0.6746,     0.67529,     0.67635,     0.67806,     0.68006,      0.6831,     0.68467,     0.68744,     0.68817,      0.6889,     0.68963,\n            0.69035,     0.69106,     0.69382,      0.6961,     0.69685,      0.6976,     0.69839,     0.69929,      0.7002,     0.70358,     0.70476,     0.70514,     0.71012,     0.71108,     0.71255,      0.7153,     0.71662,     0.71954,     0.72103,     0.72208,     0.72303,     0.72389,     0.72457,\n            0.72524,     0.72592,     0.72745,     0.72882,     0.72961,     0.73041,     0.73119,     0.73196,     0.73273,     0.73344,     0.73397,     0.73449,     0.73501,     0.73553,      0.7361,      0.7367,     0.73731,     0.73791,     0.74076,     0.74463,     0.74639,     0.74769,     0.74847,\n            0.74881,     0.74914,     0.74947,     0.74981,     0.75014,     0.75047,      0.7508,      0.7519,     0.75314,     0.75389,     0.75449,     0.75508,     0.75568,     0.75618,     0.75655,     0.75692,     0.75728,     0.75765,     0.75802,     0.75839,     0.76325,     0.76407,     0.76428,\n            0.76449,      0.7647,     0.76491,     0.76511,     0.76532,     0.76553,     0.76574,     0.76594,     0.76615,     0.76636,     0.76659,     0.76708,     0.76757,     0.76805,     0.76854,     0.76903,     0.77032,      0.7721,     0.77341,     0.77472,     0.77604,     0.77736,     0.77909,\n            0.78035,     0.78086,     0.78138,      0.7819,     0.78242,     0.78575,     0.78704,     0.78832,     0.79002,     0.79178,     0.79346,     0.79431,     0.79448,     0.79465,     0.79481,     0.79498,     0.79515,     0.79531,     0.79548,     0.79564,     0.79581,     0.79598,     0.79614,\n            0.79631,     0.79648,     0.79664,     0.79681,     0.79697,     0.80007,     0.80039,     0.80071,     0.80103,     0.80135,     0.80167,     0.80199,     0.80231,     0.80263,     0.80301,     0.80422,     0.80543,     0.80605,     0.80634,     0.80663,     0.80692,      0.8072,     0.80749,\n            0.80778,     0.80807,     0.80836,     0.80865,     0.80898,     0.80936,     0.80974,     0.81012,     0.81051,     0.81089,     0.81127,     0.81165,     0.81233,     0.81321,     0.81409,     0.81497,     0.81586,     0.81675,     0.81763,      0.8221,     0.82367,     0.82437,     0.82486,\n            0.82535,     0.82583,     0.82632,     0.82681,     0.82693,     0.82663,     0.82633,     0.82604,     0.82574,     0.82544,     0.82515,     0.82485,     0.82455,     0.82425,     0.82395,     0.82366,     0.82336,     0.82306,     0.82276,     0.82313,     0.82396,     0.82478,      0.8256,\n            0.82692,     0.82835,     0.82979,     0.83123,      0.8312,     0.82917,     0.82764,     0.82787,     0.82811,     0.82834,     0.82857,     0.82881,     0.82904,     0.82927,     0.82951,     0.82974,     0.82997,      0.8302,     0.83044,     0.83067,     0.83134,     0.83236,     0.83337,\n            0.83473,     0.83662,     0.83727,     0.83736,     0.83746,     0.83755,     0.83764,     0.83773,     0.83782,     0.83791,     0.83801,      0.8381,     0.83819,     0.83828,     0.83837,     0.83846,     0.83856,     0.83865,     0.83874,     0.83883,     0.83892,     0.83901,      0.8391,\n             0.8392,     0.83929,     0.83938,     0.83947,     0.83956,     0.83965,     0.83974,     0.83984,     0.83993,     0.84002,     0.84011,      0.8402,     0.84029,     0.84038,     0.84058,     0.84192,     0.84327,     0.84389,      0.8441,     0.84432,     0.84454,     0.84475,     0.84497,\n            0.84518,      0.8454,     0.84561,     0.84583,     0.84604,     0.84626,     0.84647,     0.84669,      0.8469,     0.84723,     0.84785,     0.84848,      0.8491,     0.84972,     0.85034,     0.85109,     0.85186,     0.85262,     0.85339,     0.85398,     0.85442,     0.85485,     0.85529,\n            0.85572,     0.85615,     0.85659,     0.85702,     0.85741,     0.85779,     0.85817,     0.85855,     0.85893,     0.85931,     0.85969,     0.86006,     0.86044,      0.8606,     0.86067,     0.86073,     0.86079,     0.86086,     0.86092,     0.86098,     0.86105,     0.86111,     0.86117,\n            0.86124,      0.8613,     0.86136,     0.86143,     0.86149,     0.86155,     0.86162,     0.86168,     0.86174,     0.86181,     0.86187,     0.86193,       0.862,     0.86206,     0.86212,     0.86219,     0.86225,     0.86231,     0.86238,     0.86244,      0.8625,     0.86257,     0.86263,\n            0.86269,     0.86276,     0.86282,     0.86288,     0.86295,     0.86301,     0.86307,     0.86314,      0.8632,     0.86326,     0.86332,     0.86339,     0.86345,     0.86351,     0.86358,     0.86364,      0.8637,     0.86377,     0.86383,     0.86389,     0.86396,     0.86404,     0.86418,\n            0.86433,     0.86447,     0.86461,     0.86475,     0.86489,     0.86503,     0.86517,     0.86531,     0.86545,      0.8656,     0.86574,     0.86588,     0.86602,     0.86616,      0.8663,     0.86644,     0.86658,     0.86672,     0.86686,       0.867,     0.86714,     0.86728,     0.86742,\n            0.86777,     0.86822,     0.86867,     0.86912,     0.86957,     0.87001,     0.87046,     0.87091,     0.87126,      0.8716,     0.87193,     0.87227,     0.87261,     0.87294,     0.87328,     0.87361,     0.87395,     0.87429,     0.87472,     0.87533,     0.87594,     0.87655,     0.87715,\n            0.87776,     0.88171,     0.88187,     0.88202,     0.88218,     0.88233,     0.88249,     0.88264,      0.8828,     0.88295,     0.88311,     0.88326,     0.88341,     0.88357,     0.88372,     0.88388,     0.88403,     0.88419,     0.88434,     0.88449,     0.88465,      0.8848,     0.88496,\n            0.88511,     0.88528,     0.88559,      0.8859,     0.88622,     0.88653,     0.88684,     0.88715,     0.88746,     0.88777,     0.88808,     0.88839,      0.8887,     0.88894,     0.88905,     0.88917,     0.88929,     0.88941,     0.88953,     0.88964,     0.88976,     0.88988,        0.89,\n            0.89012,     0.89023,     0.89035,     0.89047,     0.89059,      0.8907,     0.89082,     0.89094,     0.89106,     0.89117,     0.89129,     0.89141,     0.89153,     0.89164,     0.89176,     0.89188,       0.892,     0.89211,     0.89223,     0.89235,     0.89247,     0.89213,     0.88978,\n            0.88806,     0.88844,     0.88882,      0.8892,     0.88958,     0.88995,     0.89033,     0.89071,     0.89109,     0.89147,     0.89197,     0.89261,     0.89325,     0.89389,     0.89452,     0.89516,      0.8966,     0.89853,     0.89863,     0.89784,     0.89704,     0.89625,     0.89545,\n            0.89466,     0.89611,     0.89805,     0.89853,     0.89878,     0.89903,     0.89929,     0.89954,     0.89979,     0.90004,      0.9003,     0.90055,      0.9008,     0.90106,     0.90131,     0.90156,     0.90181,     0.90206,     0.90221,     0.90231,     0.90242,     0.90253,     0.90264,\n            0.90274,     0.90285,     0.90296,     0.90306,     0.90317,     0.90328,     0.90339,     0.90349,      0.9036,     0.90371,     0.90381,     0.90392,     0.90403,     0.90413,     0.90424,     0.90435,     0.90445,     0.90456,     0.90467,     0.90477,     0.90488,     0.90499,     0.90509,\n             0.9052,     0.90531,     0.90541,     0.90552,     0.90563,     0.90573,     0.90584,     0.90595,     0.90509,     0.90372,     0.90234,     0.90144,     0.90211,     0.90277,     0.90344,      0.9041,     0.90476,     0.90526,     0.90547,     0.90569,     0.90591,     0.90613,     0.90634,\n            0.90656,     0.90678,       0.907,     0.90721,     0.90743,     0.90765,     0.90786,     0.90808,      0.9083,     0.90851,     0.90873,     0.90895,     0.90943,     0.91044,     0.91146,     0.91247,      0.9131,     0.91324,     0.91337,      0.9135,     0.91364,     0.91377,     0.91391,\n            0.91404,     0.91418,     0.91431,     0.91445,     0.91458,     0.91471,     0.91485,     0.91498,     0.91512,     0.91525,     0.91538,     0.91552,     0.91565,     0.91578,     0.91592,     0.91605,     0.91619,     0.91632,     0.91645,     0.91659,     0.91672,     0.91685,     0.91699,\n            0.92137,     0.92183,     0.92229,     0.92276,     0.92322,     0.92368,     0.92414,      0.9246,     0.92506,     0.92387,     0.92248,     0.92108,     0.92016,     0.91975,     0.91934,     0.91893,     0.91852,     0.91811,      0.9177,     0.91729,     0.91688,     0.91647,     0.91606,\n            0.91565,     0.91511,     0.91452,     0.91394,     0.91336,     0.91277,     0.91219,      0.9116,     0.91102,     0.91095,     0.91145,     0.91194,     0.91243,     0.91293,     0.91342,     0.91391,      0.9144,     0.91475,     0.91449,     0.91424,     0.91398,     0.91373,     0.91347,\n            0.91321,     0.91296,      0.9127,     0.91244,     0.91219,     0.91193,     0.91167,     0.91141,     0.91116,      0.9109,     0.91064,     0.91038,     0.91013,      0.9098,     0.90913,     0.90846,     0.90778,     0.90711,     0.90643,     0.90576,     0.90508,     0.90452,     0.90399,\n            0.90345,     0.90292,     0.90238,     0.90184,      0.9013,     0.90077,     0.90023,     0.89579,      0.8972,      0.8986,     0.90001,     0.90143,     0.90284,     0.89561,      0.8921,     0.88856,     0.88899,     0.89042,     0.89184,     0.89124,     0.89035,     0.88946,     0.88857,\n            0.88767,     0.88673,     0.88313,     0.88031,     0.87813,     0.87067,     0.86944,      0.8682,     0.86697,     0.86572,     0.86643,     0.86786,     0.86928,     0.87188,     0.87337,     0.87212,     0.87087,     0.86962,     0.86836,      0.8677,     0.86707,     0.86644,     0.86581,\n            0.86517,     0.86454,     0.86391,     0.86327,     0.86211,     0.85829,     0.84738,     0.84532,      0.8447,     0.84408,     0.84346,     0.84284,     0.84221,     0.84159,     0.84096,     0.84034,     0.83727,     0.83223,     0.82824,      0.8258,     0.82336,     0.81877,     0.81483,\n            0.81234,     0.80984,     0.80733,     0.80481,     0.80105,     0.79624,     0.79092,     0.78833,     0.78573,     0.78164,      0.7764,     0.77139,     0.76872,     0.76603,     0.75939,     0.75742,     0.75547,     0.75351,     0.74479,      0.7428,      0.7408,     0.73796,      0.7317,\n            0.72885,     0.72599,     0.71667,     0.71459,      0.7125,     0.69888,      0.6812,     0.67949,     0.67778,     0.67606,     0.67433,      0.6705,      0.6588,     0.65098,     0.64695,     0.63452,     0.61887,     0.61733,      0.6158,     0.61425,     0.61271,     0.61116,     0.60864,\n            0.60578,     0.60291,     0.59525,     0.56904,     0.56631,      0.5645,     0.56267,     0.56084,     0.55901,     0.55289,     0.53921,     0.53606,     0.53289,     0.52183,     0.51796,     0.51407,     0.50838,     0.49133,     0.48121,     0.47095,     0.45129,     0.44419,     0.43405,\n            0.42314,     0.40442,     0.39767,     0.39198,     0.38677,     0.38216,     0.37737,     0.36511,     0.35127,     0.34524,     0.31667,     0.31309,     0.30949,     0.30559,      0.3005,     0.29538,     0.26748,     0.26592,     0.26436,     0.26279,     0.26122,     0.25965,     0.25807,\n            0.25649,     0.24316,     0.22676,     0.22444,     0.22211,     0.21978,     0.21745,     0.21426,     0.20482,     0.19779,     0.19203,     0.18434,     0.16607,     0.13245,      0.1242,     0.11797,     0.11319,     0.11134,     0.10949,     0.10763,     0.10576,      0.1039,     0.10202,\n            0.10015,    0.098236,    0.095569,    0.092894,    0.090212,    0.087523,    0.084826,    0.076343,    0.062775,    0.057232,    0.051658,    0.043859,    0.035369,    0.033309,    0.032597,    0.031884,    0.031171,    0.030457,    0.029742,    0.029028,    0.028312,    0.027596,     0.02688,\n           0.026163,    0.025445,    0.024727,    0.024009,     0.02329,     0.02257,     0.02185,    0.021129,    0.020408,    0.019687,    0.018964,    0.018242,    0.017518,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.10648,     0.10648,     0.15519,      0.1949,     0.22312,     0.24903,     0.27383,      0.2846,     0.29777,     0.31694,     0.32853,     0.34146,      0.3492,     0.36283,     0.37028,     0.37978,     0.38202,     0.38814,     0.39123,      0.4024,     0.40795,      0.4187,     0.42194,\n            0.43029,     0.43333,      0.4381,     0.43935,     0.44152,     0.44812,     0.45183,     0.45909,     0.46058,     0.46486,     0.46531,     0.46576,      0.4662,     0.46665,     0.46986,      0.4721,     0.47619,     0.47776,     0.47957,     0.48629,     0.48861,     0.49207,     0.50412,\n             0.5098,     0.51184,     0.51246,     0.51309,     0.51372,     0.51543,     0.51776,     0.51894,     0.51934,     0.51973,     0.52013,     0.52053,     0.52093,     0.52175,     0.52301,     0.52506,     0.52746,     0.53113,     0.53303,      0.5364,     0.53729,     0.53817,     0.53906,\n            0.53994,     0.54082,     0.54421,     0.54701,     0.54794,     0.54886,     0.54985,     0.55097,     0.55209,     0.55631,     0.55939,     0.56123,     0.56757,     0.56881,     0.57068,     0.57422,     0.57593,     0.57971,     0.58164,     0.58301,     0.58425,     0.58538,     0.58626,\n            0.58715,     0.58803,     0.59005,     0.59185,      0.5929,     0.59394,     0.59498,       0.596,     0.59702,     0.59797,     0.59867,     0.59936,     0.60006,     0.60076,     0.60151,     0.60232,     0.60313,     0.60393,     0.60776,     0.61299,     0.61537,     0.61714,     0.61822,\n            0.61867,     0.61913,     0.61958,     0.62003,     0.62049,     0.62094,      0.6214,     0.62291,     0.62461,     0.62564,     0.62646,     0.62729,     0.62811,      0.6288,     0.62931,     0.62982,     0.63033,     0.63084,     0.63135,     0.63186,     0.63864,     0.63979,     0.64008,\n            0.64038,     0.64067,     0.64096,     0.64125,     0.64154,     0.64184,     0.64213,     0.64242,     0.64271,     0.64301,     0.64333,     0.64402,     0.64471,      0.6454,     0.64609,     0.64678,      0.6486,     0.65112,     0.65299,     0.65486,     0.65675,     0.65864,     0.66113,\n            0.66295,     0.66369,     0.66444,     0.66519,     0.66594,     0.67079,     0.67266,     0.67453,     0.67704,     0.67963,      0.6821,     0.68336,     0.68361,     0.68386,      0.6841,     0.68435,      0.6846,     0.68484,     0.68509,     0.68534,     0.68558,     0.68583,     0.68608,\n            0.68632,     0.68657,     0.68682,     0.68706,     0.68731,     0.69193,     0.69241,     0.69289,     0.69337,     0.69385,     0.69432,      0.6948,     0.69528,     0.69576,     0.69634,     0.69816,     0.69998,     0.70092,     0.70136,     0.70179,     0.70223,     0.70267,     0.70311,\n            0.70355,     0.70399,     0.70442,     0.70486,     0.70536,     0.70594,     0.70652,     0.70711,     0.70769,     0.70827,     0.70886,     0.70944,     0.71047,     0.71182,     0.71317,     0.71453,     0.71589,     0.71726,     0.71863,     0.72555,     0.72801,      0.7291,     0.72986,\n            0.73063,      0.7314,     0.73216,     0.73293,     0.73328,     0.73316,     0.73304,     0.73292,      0.7328,     0.73268,     0.73256,     0.73243,     0.73231,     0.73219,     0.73207,     0.73195,     0.73183,     0.73171,     0.73159,     0.73232,     0.73362,     0.73493,     0.73624,\n            0.73834,     0.74062,     0.74292,     0.74524,     0.74624,     0.74544,     0.74491,     0.74529,     0.74567,     0.74605,     0.74643,     0.74681,     0.74719,     0.74756,     0.74794,     0.74832,      0.7487,     0.74908,     0.74946,     0.74984,     0.75094,     0.75259,     0.75425,\n            0.75648,      0.7596,     0.76067,     0.76082,     0.76097,     0.76112,     0.76127,     0.76143,     0.76158,     0.76173,     0.76188,     0.76203,     0.76218,     0.76233,     0.76249,     0.76264,     0.76279,     0.76294,     0.76309,     0.76324,     0.76339,     0.76355,      0.7637,\n            0.76385,       0.764,     0.76415,      0.7643,     0.76445,     0.76461,     0.76476,     0.76491,     0.76506,     0.76521,     0.76536,     0.76552,     0.76567,     0.76582,     0.76614,     0.76838,     0.77063,     0.77166,     0.77202,     0.77238,     0.77274,     0.77311,     0.77347,\n            0.77383,     0.77419,     0.77455,     0.77491,     0.77527,     0.77563,     0.77599,     0.77636,     0.77672,     0.77727,     0.77832,     0.77937,     0.78042,     0.78147,     0.78251,     0.78379,     0.78509,      0.7864,      0.7877,     0.78871,     0.78945,     0.79019,     0.79094,\n            0.79168,     0.79242,     0.79316,     0.79391,     0.79458,     0.79524,     0.79589,     0.79654,     0.79719,     0.79784,      0.7985,     0.79915,      0.7998,     0.80008,     0.80019,      0.8003,     0.80041,     0.80051,     0.80062,     0.80073,     0.80084,     0.80095,     0.80106,\n            0.80117,     0.80128,     0.80139,      0.8015,     0.80161,     0.80172,     0.80183,     0.80194,     0.80205,     0.80216,     0.80227,     0.80238,     0.80249,      0.8026,     0.80271,     0.80282,     0.80293,     0.80304,     0.80315,     0.80326,     0.80337,     0.80348,     0.80359,\n             0.8037,     0.80381,     0.80392,     0.80403,     0.80414,     0.80425,     0.80436,     0.80447,     0.80458,     0.80469,      0.8048,     0.80491,     0.80502,     0.80513,     0.80524,     0.80534,     0.80545,     0.80556,     0.80567,     0.80578,     0.80589,     0.80604,     0.80629,\n            0.80654,     0.80678,     0.80703,     0.80727,     0.80752,     0.80777,     0.80801,     0.80826,      0.8085,     0.80875,       0.809,     0.80924,     0.80949,     0.80973,     0.80998,     0.81023,     0.81047,     0.81072,     0.81096,     0.81121,     0.81146,      0.8117,     0.81195,\n            0.81256,     0.81335,     0.81413,     0.81492,     0.81571,      0.8165,     0.81729,     0.81808,      0.8187,     0.81929,     0.81989,     0.82048,     0.82108,     0.82167,     0.82227,     0.82287,     0.82346,     0.82406,     0.82484,     0.82592,       0.827,     0.82809,     0.82917,\n            0.83025,     0.83736,     0.83763,     0.83791,     0.83819,     0.83847,     0.83875,     0.83903,     0.83931,     0.83959,     0.83987,     0.84015,     0.84043,     0.84071,     0.84099,     0.84127,     0.84155,     0.84183,      0.8421,     0.84238,     0.84266,     0.84294,     0.84322,\n             0.8435,     0.84381,     0.84438,     0.84495,     0.84551,     0.84608,     0.84665,     0.84722,     0.84778,     0.84835,     0.84892,     0.84949,     0.85005,     0.85048,      0.8507,     0.85091,     0.85113,     0.85135,     0.85156,     0.85178,     0.85199,     0.85221,     0.85243,\n            0.85264,     0.85286,     0.85307,     0.85329,     0.85351,     0.85372,     0.85394,     0.85416,     0.85437,     0.85459,      0.8548,     0.85502,     0.85524,     0.85545,     0.85567,     0.85589,      0.8561,     0.85632,     0.85653,     0.85675,     0.85697,     0.85704,     0.85645,\n            0.85616,     0.85687,     0.85758,     0.85829,     0.85899,      0.8597,     0.86041,     0.86112,     0.86183,     0.86253,     0.86347,     0.86467,     0.86587,     0.86707,     0.86827,     0.86947,      0.8722,     0.87585,     0.87693,     0.87676,     0.87659,     0.87641,     0.87624,\n            0.87606,     0.87909,     0.88284,     0.88376,     0.88425,     0.88474,     0.88523,     0.88572,     0.88621,      0.8867,      0.8872,     0.88769,     0.88818,     0.88867,     0.88916,     0.88965,     0.89014,     0.89063,     0.89091,     0.89112,     0.89133,     0.89154,     0.89175,\n            0.89196,     0.89217,     0.89238,     0.89259,     0.89279,       0.893,     0.89321,     0.89342,     0.89363,     0.89384,     0.89405,     0.89426,     0.89447,     0.89468,     0.89489,     0.89509,      0.8953,     0.89551,     0.89572,     0.89593,     0.89614,     0.89635,     0.89656,\n            0.89677,     0.89698,     0.89719,      0.8974,      0.8976,     0.89781,     0.89802,     0.89823,     0.89814,     0.89789,     0.89763,     0.89775,     0.89907,     0.90039,     0.90171,     0.90303,     0.90435,     0.90534,     0.90577,     0.90621,     0.90665,     0.90708,     0.90752,\n            0.90796,     0.90839,     0.90883,     0.90926,      0.9097,     0.91014,     0.91057,     0.91101,     0.91144,     0.91188,     0.91232,     0.91275,     0.91373,     0.91578,     0.91783,     0.91988,     0.92117,     0.92144,     0.92172,     0.92199,     0.92227,     0.92254,     0.92281,\n            0.92309,     0.92336,     0.92364,     0.92391,     0.92418,     0.92446,     0.92473,     0.92501,     0.92528,     0.92555,     0.92583,      0.9261,     0.92638,     0.92665,     0.92692,      0.9272,     0.92747,     0.92774,     0.92802,     0.92829,     0.92857,     0.92884,     0.92911,\n            0.93815,     0.93911,     0.94007,     0.94103,       0.942,     0.94296,     0.94392,     0.94488,     0.94584,     0.94582,     0.94567,     0.94553,     0.94543,     0.94539,     0.94535,     0.94531,     0.94526,     0.94522,     0.94518,     0.94513,     0.94509,     0.94505,     0.94501,\n            0.94496,     0.94491,     0.94485,     0.94478,     0.94472,     0.94466,      0.9446,     0.94454,     0.94448,     0.94496,     0.94602,     0.94708,     0.94815,     0.94921,     0.95028,     0.95134,     0.95241,     0.95327,     0.95324,     0.95322,      0.9532,     0.95317,     0.95315,\n            0.95313,      0.9531,     0.95308,     0.95306,     0.95303,     0.95301,     0.95299,     0.95297,     0.95294,     0.95292,      0.9529,     0.95287,     0.95285,     0.95282,     0.95276,      0.9527,     0.95264,     0.95257,     0.95251,     0.95245,     0.95239,     0.95234,     0.95229,\n            0.95224,     0.95219,     0.95214,     0.95209,     0.95204,     0.95199,     0.95194,      0.9533,     0.95649,     0.95968,      0.9629,     0.96615,     0.96941,     0.96985,     0.96964,     0.96943,     0.97212,     0.97553,     0.97894,     0.97935,     0.97931,     0.97928,     0.97924,\n             0.9792,     0.97916,     0.97901,      0.9789,      0.9788,     0.97849,     0.97844,     0.97838,     0.97833,     0.97828,     0.98093,      0.9846,     0.98827,     0.99501,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.99138,     0.99138,     0.99138,     0.99138,     0.99138,     0.99138,     0.99138,     0.99138,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.98276,     0.97414,     0.97414,     0.97414,     0.97414,     0.97414,\n            0.97414,     0.97414,     0.97414,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96552,     0.96403,\n             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9522,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,     0.94828,\n            0.94828,     0.94828,     0.94828,     0.94828,       0.948,     0.94742,     0.94685,     0.94627,     0.94569,     0.94511,     0.94453,     0.94395,     0.94337,     0.94279,     0.94221,     0.94163,     0.94105,     0.94047,     0.93989,     0.93966,     0.93966,     0.93966,     0.93966,\n            0.93966,     0.93966,     0.93966,     0.93966,       0.938,     0.93407,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,\n            0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93103,     0.93022,     0.92581,\n            0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92241,     0.92142,     0.91995,     0.91848,     0.91701,     0.91553,\n            0.91406,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,\n            0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,\n            0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91379,     0.91215,     0.90963,     0.90711,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,\n            0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,\n            0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,\n            0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90517,     0.90291,     0.90039,     0.89786,      0.8962,     0.89546,     0.89473,     0.89399,     0.89325,     0.89252,     0.89178,     0.89104,     0.89031,     0.88957,     0.88883,\n             0.8881,     0.88713,     0.88609,     0.88505,     0.88401,     0.88297,     0.88193,     0.88089,     0.87985,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87931,     0.87923,     0.87877,     0.87832,     0.87787,     0.87741,     0.87696,\n            0.87651,     0.87605,      0.8756,     0.87515,     0.87469,     0.87424,     0.87379,     0.87333,     0.87288,     0.87243,     0.87197,     0.87152,     0.87107,      0.8705,     0.86932,     0.86814,     0.86696,     0.86578,     0.86461,     0.86343,     0.86225,     0.86128,     0.86035,\n            0.85942,     0.85849,     0.85756,     0.85663,      0.8557,     0.85477,     0.85384,     0.84483,     0.84483,     0.84483,     0.84483,     0.84483,     0.84483,     0.83193,     0.82604,     0.82015,     0.81897,     0.81897,     0.81897,     0.81768,     0.81621,     0.81474,     0.81326,\n            0.81179,     0.81024,     0.80434,     0.79976,     0.79623,     0.78425,     0.78229,     0.78033,     0.77836,      0.7764,     0.77586,     0.77586,     0.77586,     0.77586,      0.7752,     0.77324,     0.77128,     0.76931,     0.76735,     0.76631,     0.76533,     0.76435,     0.76337,\n            0.76239,      0.7614,     0.76042,     0.75944,     0.75765,     0.75175,     0.73517,     0.73208,     0.73115,     0.73022,     0.72929,     0.72836,     0.72743,      0.7265,     0.72557,     0.72464,     0.72009,     0.71267,     0.70683,     0.70329,     0.69976,     0.69315,     0.68752,\n            0.68398,     0.68045,     0.67691,     0.67338,     0.66813,     0.66146,     0.65415,     0.65062,     0.64709,     0.64155,     0.63452,     0.62786,     0.62433,     0.62079,     0.61211,     0.60956,     0.60703,     0.60451,     0.59336,     0.59084,     0.58831,     0.58474,     0.57691,\n            0.57338,     0.56984,     0.55845,     0.55592,      0.5534,     0.53714,     0.51653,     0.51457,      0.5126,     0.51064,     0.50868,     0.50433,      0.4912,     0.48256,     0.47814,     0.46469,     0.44809,     0.44648,     0.44487,     0.44327,     0.44166,     0.44005,     0.43744,\n            0.43449,     0.43155,     0.42374,     0.39766,     0.39501,     0.39324,     0.39147,      0.3897,     0.38794,     0.38206,     0.36912,     0.36617,     0.36323,     0.35303,     0.34949,     0.34596,     0.34082,     0.32567,     0.31684,       0.308,      0.2914,     0.28551,     0.27718,\n            0.26835,     0.25347,     0.24818,     0.24376,     0.23975,     0.23622,     0.23257,     0.22333,     0.21305,     0.20863,     0.18812,      0.1856,     0.18307,     0.18035,     0.17682,     0.17329,     0.15439,     0.15335,     0.15231,     0.15127,     0.15023,     0.14919,     0.14815,\n            0.14711,     0.13841,     0.12788,      0.1264,     0.12493,     0.12346,     0.12199,     0.11998,     0.11409,     0.10975,     0.10621,     0.10153,    0.090556,    0.070924,    0.066214,     0.06268,    0.059992,    0.058952,    0.057913,    0.056873,    0.055834,    0.054794,    0.053755,\n           0.052715,    0.051655,    0.050182,     0.04871,    0.047237,    0.045764,    0.044291,    0.039686,    0.032405,    0.029459,    0.026514,    0.022421,    0.018003,    0.016937,    0.016568,      0.0162,    0.015832,    0.015464,    0.015096,    0.014728,    0.014359,    0.013991,    0.013623,\n           0.013255,    0.012887,    0.012518,     0.01215,    0.011782,    0.011414,    0.011046,    0.010678,    0.010309,   0.0099412,    0.009573,   0.0092048,   0.0088366,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.6987911571357426\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.67023])\nnames: {0: 'Defect'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9455293833672211, 'metrics/recall(B)': 0.8978646133818547, 'metrics/mAP50(B)': 0.9558369780869106, 'metrics/mAP50-95(B)': 0.6702305103633907, 'fitness': 0.6987911571357426}\nsave_dir: PosixPath('runs/detect/train2')\nspeed: {'preprocess': 0.3508114333322333, 'inference': 20.02073865556263, 'loss': 0.0007403055507085001, 'postprocess': 1.9294847777776643}\ntask: 'detect'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your trained model (change this to the correct path)\n",
        "model_path = '/kaggle/working/yolov5/runs/detect/train2/weights/best.pt'  # Replace with the correct path to the best model\n",
        "# Evaluate the model\n",
        "model = YOLO(model_path)\n",
        "# Evaluate on the validation dataset (it automatically uses the val split from your YAML file)\n",
        "results = model.val(data=data_yaml_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T10:56:36.909211Z",
          "iopub.execute_input": "2025-03-26T10:56:36.909582Z",
          "iopub.status.idle": "2025-03-26T10:56:51.126112Z",
          "shell.execute_reply.started": "2025-03-26T10:56:36.909550Z",
          "shell.execute_reply": "2025-03-26T10:56:51.124991Z"
        },
        "id": "dpviboy9SXtw",
        "outputId": "a47ea39f-af15-442a-990e-def0d2a7ba38"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 580.68it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.953      0.876      0.955      0.635\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 2.0ms preprocess, 9.6ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1mruns/detect/val3\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model.val(data=\"/kaggle/working/data.yaml\", split=\"test\")\n",
        "\n",
        "# Extract correct metrics\n",
        "precision = metrics.box.p.mean().item()  # Mean Precision\n",
        "recall = metrics.box.r.mean().item()  # Mean Recall\n",
        "map50 = metrics.box.map50.item()  # mAP@50 (used as accuracy proxy)\n",
        "map = metrics.box.map.item()  # mAP@50-95\n",
        "\n",
        "# Calculate F1-score\n",
        "f1_score = (2 * precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "# Convert metrics to dictionary\n",
        "metrics_dict = {\n",
        "    \"Defect\": {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1-score\": f1_score,\n",
        "        \"accuracy (mAP@50)\": map50,\n",
        "        \"mAP@50-95\": map\n",
        "    }\n",
        "}\n",
        "# Pretty print the results\n",
        "print(json.dumps(metrics_dict, indent=4))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T10:56:51.127420Z",
          "iopub.execute_input": "2025-03-26T10:56:51.127779Z",
          "iopub.status.idle": "2025-03-26T10:57:07.140646Z",
          "shell.execute_reply.started": "2025-03-26T10:56:51.127753Z",
          "shell.execute_reply": "2025-03-26T10:57:07.139641Z"
        },
        "id": "79iKF1isSXtx",
        "outputId": "dce07c12-904a-4b1f-f478-59fd15d929db"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/test-def/labels... 180 images, 88 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 627.63it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/test-def is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:10<00:00,  1.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        131      0.917      0.931      0.979      0.679\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 3.5ms preprocess, 7.7ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mruns/detect/val4\u001b[0m\n{\n    \"Defect\": {\n        \"precision\": 0.9173317654877717,\n        \"recall\": 0.9312977099236641,\n        \"f1-score\": 0.9242614830839639,\n        \"accuracy (mAP@50)\": 0.9793551472067038,\n        \"mAP@50-95\": 0.6787198860377969\n    }\n}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "# Path to the test image\n",
        "image_path = \"/kaggle/input/data-shoes/test/test-def/images/aug_178_20211110_185929.jpg\"\n",
        "\n",
        "# Perform inference\n",
        "results = model.predict(image_path, save=True, conf=0.5)  # Adjust confidence threshold as needed\n",
        "\n",
        "# Display the result\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T12:14:07.645085Z",
          "iopub.status.idle": "2025-03-26T12:14:07.645466Z",
          "shell.execute_reply": "2025-03-26T12:14:07.645302Z"
        },
        "id": "CBifisuuSXtx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolo v8l"
      ],
      "metadata": {
        "id": "HI7MAoDUSXty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T10:57:07.141822Z",
          "iopub.execute_input": "2025-03-26T10:57:07.142192Z",
          "iopub.status.idle": "2025-03-26T10:57:07.281645Z",
          "shell.execute_reply.started": "2025-03-26T10:57:07.142158Z",
          "shell.execute_reply": "2025-03-26T10:57:07.280395Z"
        },
        "id": "jJkV9j4mSXty"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Check CUDA availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_yaml_path = \"/kaggle/working/data.yaml\"\n",
        "model = YOLO(\"yolov8l.pt\")  # Using YOLOv8 weights\n",
        "\n",
        "model.train(\n",
        "     data=data_yaml_path,\n",
        "    epochs=30,               # More training\n",
        "    batch=16,                # Larger batch\n",
        "    imgsz=960,               # Higher resolution\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    lr0=0.001, lrf=0.01,      # Learning rate decay\n",
        "    optimizer='SGD',         # Try SGD for detection tasks\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005,\n",
        "    cos_lr=True,             # Cyclic LR\n",
        "    augment=True,\n",
        "    freeze=[0, 1, 2],\n",
        "    amp=True                 # Enable mixed precision\n",
        ")\n",
        "\"\"\"\n",
        "    \"YOLOv5l\": \"yolov5l.pt\",\n",
        "    \"YOLOv6-L\": \"yolov6l.pt\",\n",
        "    \"YOLOv7-X\": \"yolov7x.pt\",\n",
        "    \"YOLOv8-L\": \"yolov8l.pt\",\n",
        "    \"YOLO-NAS\": \"yolo_nas_l\"\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T10:57:07.284207Z",
          "iopub.execute_input": "2025-03-26T10:57:07.284450Z",
          "iopub.status.idle": "2025-03-26T12:14:07.331144Z",
          "shell.execute_reply.started": "2025-03-26T10:57:07.284428Z",
          "shell.execute_reply": "2025-03-26T12:14:07.330043Z"
        },
        "id": "Eumn6xkWSXuA",
        "outputId": "463744e0-cd6f-4114-d95d-8b641b490d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.7M/83.7M [00:00<00:00, 125MB/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/kaggle/working/data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=960, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=[0, 1, 2], multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n 22        [15, 18, 21]  1   5583571  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \nModel summary: 209 layers, 43,630,611 parameters, 43,630,595 gradients, 165.4 GFLOPs\n\nTransferred 589/595 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\nFreezing layer 'model.0.conv.weight'\nFreezing layer 'model.0.bn.weight'\nFreezing layer 'model.0.bn.bias'\nFreezing layer 'model.1.conv.weight'\nFreezing layer 'model.1.bn.weight'\nFreezing layer 'model.1.bn.bias'\nFreezing layer 'model.2.cv1.conv.weight'\nFreezing layer 'model.2.cv1.bn.weight'\nFreezing layer 'model.2.cv1.bn.bias'\nFreezing layer 'model.2.cv2.conv.weight'\nFreezing layer 'model.2.cv2.bn.weight'\nFreezing layer 'model.2.cv2.bn.bias'\nFreezing layer 'model.2.m.0.cv1.conv.weight'\nFreezing layer 'model.2.m.0.cv1.bn.weight'\nFreezing layer 'model.2.m.0.cv1.bn.bias'\nFreezing layer 'model.2.m.0.cv2.conv.weight'\nFreezing layer 'model.2.m.0.cv2.bn.weight'\nFreezing layer 'model.2.m.0.cv2.bn.bias'\nFreezing layer 'model.2.m.1.cv1.conv.weight'\nFreezing layer 'model.2.m.1.cv1.bn.weight'\nFreezing layer 'model.2.m.1.cv1.bn.bias'\nFreezing layer 'model.2.m.1.cv2.conv.weight'\nFreezing layer 'model.2.m.1.cv2.bn.weight'\nFreezing layer 'model.2.m.1.cv2.bn.bias'\nFreezing layer 'model.2.m.2.cv1.conv.weight'\nFreezing layer 'model.2.m.2.cv1.bn.weight'\nFreezing layer 'model.2.m.2.cv1.bn.bias'\nFreezing layer 'model.2.m.2.cv2.conv.weight'\nFreezing layer 'model.2.m.2.cv2.bn.weight'\nFreezing layer 'model.2.m.2.cv2.bn.bias'\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/labels... 1440 images, 720 backgrounds, 4 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1440/1440 [00:02<00:00, 554.01it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_28_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_437_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_732_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /kaggle/input/shoe-data/Data-defect/Data-defect/train-def/images/aug_896_20211201_092710.jpg: ignoring corrupt image/label: Label class 1 exceeds dataset class count 1. Possible class labels are 0-0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/train-def is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 394.41it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Plotting labels to runs/detect/train3/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.001, momentum=0.937) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 960 train, 960 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train3\u001b[0m\nStarting training for 30 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       1/30      14.4G       3.21      6.721      3.237         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:26<00:00,  1.62s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.02s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116     0.0018      0.767    0.00703    0.00156\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       2/30      14.5G      2.622      3.449      2.763         15        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:25<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.00s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116        0.3      0.276      0.207     0.0738\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       3/30      13.9G       2.15      2.537      2.321          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:06<00:00,  1.02s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.522      0.491      0.496      0.215\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       4/30        14G      1.885      1.971      1.997         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.689      0.631      0.676      0.308\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       5/30        14G      1.755      1.585      1.872         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.768      0.672      0.761      0.384\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       6/30        14G      1.603      1.261      1.697         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.821      0.733      0.812      0.433\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       7/30        14G      1.524      1.074      1.609         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.818      0.733      0.825      0.436\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       8/30        14G      1.428     0.9181      1.522         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.881      0.828      0.897      0.519\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "       9/30        14G      1.375     0.8746      1.502         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.881      0.853      0.907      0.519\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      10/30        14G      1.271     0.8003      1.398         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.928      0.883      0.942      0.581\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      11/30        14G      1.257     0.7588       1.37         11        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.914      0.888      0.948      0.576\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      12/30        14G      1.168     0.7075      1.282          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.898       0.91      0.945       0.59\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      13/30        14G      1.129     0.6288      1.272         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.955      0.862      0.935      0.603\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      14/30        14G      1.091     0.6263      1.253          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.962      0.884      0.941      0.587\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      15/30        14G      1.038     0.6098      1.216          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.05it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.944       0.94      0.969      0.618\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      16/30        14G     0.9917     0.5598       1.19         13        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.02it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.926      0.869      0.947      0.616\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      17/30        14G     0.9715     0.5321      1.177         19        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.952      0.914      0.965      0.646\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      18/30        14G     0.9642     0.5298      1.161         14        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.946      0.922      0.978      0.645\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      19/30        14G     0.9089     0.5109      1.135         10        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.964      0.927      0.977      0.647\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      20/30        14G      0.892     0.4864      1.119         12        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.61s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.963       0.94      0.967      0.631\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      21/30        14G     0.8548      0.417       1.11          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:28<00:00,  1.65s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.922      0.948       0.97      0.669\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      22/30        14G     0.8064     0.4014      1.075          8        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.953      0.922      0.962      0.669\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      23/30        14G     0.7602     0.3821       1.05          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.961      0.914      0.969      0.697\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      24/30        14G     0.7386      0.366      1.029          5        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116       0.97      0.905      0.972      0.695\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      25/30        14G     0.6902     0.3315      1.013          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.964      0.921       0.97      0.699\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      26/30        14G     0.7298     0.3483      1.036          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.957      0.952      0.975      0.708\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      27/30        14G     0.6704     0.3272     0.9934          6        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.963      0.957      0.976      0.689\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      28/30        14G     0.6857     0.3278      1.007          3        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.961      0.957      0.978      0.715\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      29/30        14G     0.6636     0.3258     0.9906          7        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.03it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.965      0.955      0.978      0.698\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "      30/30        14G     0.6558     0.3145      0.992          9        960: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [02:24<00:00,  1.60s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.04it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.965      0.956      0.978      0.703\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n30 epochs completed in 1.272 hours.\nOptimizer stripped from runs/detect/train3/weights/last.pt, 87.7MB\nOptimizer stripped from runs/detect/train3/weights/best.pt, 87.7MB\n\nValidating runs/detect/train3/weights/best.pt...\nUltralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 112 layers, 43,607,379 parameters, 0 gradients, 164.8 GFLOPs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.19s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.933      0.966      0.982      0.737\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 0.3ms preprocess, 68.3ms inference, 0.0ms loss, 1.3ms postprocess per image\nResults saved to \u001b[1mruns/detect/train3\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n    \"YOLOv5l\": \"yolov5l.pt\",\\n    \"YOLOv6-L\": \"yolov6l.pt\",\\n    \"YOLOv7-X\": \"yolov7x.pt\",\\n    \"YOLOv8-L\": \"yolov8l.pt\",\\n    \"YOLO-NAS\": \"yolo_nas_l\"\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your trained model (change this to the correct path)\n",
        "model_path = '/kaggle/working/yolov5/runs/detect/train3/weights/best.pt'  # Replace with the correct path to the best model\n",
        "# Evaluate the model\n",
        "model = YOLO(model_path)\n",
        "# Evaluate on the validation dataset (it automatically uses the val split from your YAML file)\n",
        "results = model.val(data=data_yaml_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T13:19:40.851730Z",
          "iopub.execute_input": "2025-03-26T13:19:40.852254Z",
          "iopub.status.idle": "2025-03-26T13:20:07.087668Z",
          "shell.execute_reply.started": "2025-03-26T13:19:40.852223Z",
          "shell.execute_reply": "2025-03-26T13:20:07.086396Z"
        },
        "id": "pGFbtD9tSXuB",
        "outputId": "9289d8f3-8bf0-423c-8226-74bcf816ef77"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 112 layers, 43,607,379 parameters, 0 gradients, 164.8 GFLOPs\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 3.99MB/s]\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/val/labels... 180 images, 92 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:01<00:00, 123.51it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/val is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        116      0.961      0.957      0.978      0.715\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 3.1ms preprocess, 29.8ms inference, 0.0ms loss, 2.6ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"/kaggle/working/runsfolderfull\", 'zip', \"/kaggle/working/yolov5/runs\")\n",
        "print(\"Runs folder saved as runs_backup.zip in Kaggle working directory.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T13:25:48.537326Z",
          "iopub.execute_input": "2025-03-26T13:25:48.537650Z",
          "iopub.status.idle": "2025-03-26T13:26:09.282047Z",
          "shell.execute_reply.started": "2025-03-26T13:25:48.537623Z",
          "shell.execute_reply": "2025-03-26T13:26:09.281070Z"
        },
        "id": "WqgCiP1QSXuB",
        "outputId": "152858b4-b913-4a3a-8af5-ae89ede800a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Runs folder saved as runs_backup.zip in Kaggle working directory.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model.val(data=\"/kaggle/working/data.yaml\", split=\"test\")\n",
        "import json\n",
        "# Extract correct metrics\n",
        "precision = metrics.box.p.mean().item()  # Mean Precision\n",
        "recall = metrics.box.r.mean().item()  # Mean Recall\n",
        "map50 = metrics.box.map50.item()  # mAP@50 (used as accuracy proxy)\n",
        "map = metrics.box.map.item()  # mAP@50-95\n",
        "\n",
        "# Calculate F1-score\n",
        "f1_score = (2 * precision * recall) / (precision + recall + 1e-6)\n",
        "\n",
        "# Convert metrics to dictionary\n",
        "metrics_dict = {\n",
        "    \"Defect\": {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1-score\": f1_score,\n",
        "        \"accuracy (mAP@50)\": map50,\n",
        "        \"mAP@50-95\": map\n",
        "    }\n",
        "}\n",
        "# Pretty print the results\n",
        "print(json.dumps(metrics_dict, indent=4))"
      ],
      "metadata": {
        "trusted": true,
        "id": "tD4CtcIjl98Q",
        "execution": {
          "iopub.status.busy": "2025-03-26T13:20:40.811707Z",
          "iopub.execute_input": "2025-03-26T13:20:40.812107Z",
          "iopub.status.idle": "2025-03-26T13:20:57.719484Z",
          "shell.execute_reply.started": "2025-03-26T13:20:40.812074Z",
          "shell.execute_reply": "2025-03-26T13:20:57.718489Z"
        },
        "outputId": "7f8381bb-eca2-4389-b393-2433e0bb5a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ultralytics 8.3.96 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/shoe-data/Data-defect/Data-defect/test-def/labels... 180 images, 88 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:00<00:00, 648.71it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/shoe-data/Data-defect/Data-defect/test-def is not writeable, cache not saved.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.03it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "                   all        180        131      0.967      0.947      0.971       0.75\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Speed: 4.0ms preprocess, 29.7ms inference, 0.0ms loss, 1.0ms postprocess per image\nResults saved to \u001b[1mruns/detect/val3\u001b[0m\n{\n    \"Defect\": {\n        \"precision\": 0.9674844949047937,\n        \"recall\": 0.9465648854961832,\n        \"f1-score\": 0.956909869788317,\n        \"accuracy (mAP@50)\": 0.9714932592415539,\n        \"mAP@50-95\": 0.7502007537469786\n    }\n}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "J0x8qAENSXuD"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}